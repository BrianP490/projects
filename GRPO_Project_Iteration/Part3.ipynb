{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg4EjgNMYmSn"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "PPO Loss\n",
        "\n",
        "* Terrrible loss, very one sided predictions\n",
        "* Needs Actor to provide rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K8F-BVwxNvrC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4D4bl4-xNwvl"
      },
      "outputs": [],
      "source": [
        "# Define the neural network\n",
        "class LogicNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogicNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 4)  # Input layer -> Hidden Layer\n",
        "        self.fc2 = nn.Linear(4, 1)  # Hidden Layer -> Output Layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        logits = self.fc2(x)\n",
        "        return logits\n",
        "\n",
        "    def get_action_and_or_log_prob(self, state, action=None):\n",
        "        \"\"\"Helper method to get action and its log_prob from logits\"\"\"\n",
        "        logits = self.forward(state)    # Get the logits from a forward pass of the Policy Network\n",
        "        # For a binary output (0 or 1), Bernoulli distribution is appropriate\n",
        "        probs = torch.distributions.Bernoulli(logits=logits)\n",
        "\n",
        "        if action is None:\n",
        "            sampled_action = probs.sample() # Sample action based on current probabilities (returns 0 or 1)\n",
        "            log_prob = probs.log_prob(sampled_action)   # Calculate the log of the probability the sampled action is chosen\n",
        "            return sampled_action, log_prob\n",
        "        else:\n",
        "            log_prob = probs.log_prob(action)       # returns the log of the probability the action is chosen\n",
        "            return log_prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "50L6eW4tNxrC"
      },
      "outputs": [],
      "source": [
        "# Define the environment\n",
        "class LogicGateEnv:\n",
        "    def __init__(self, gate=\"AND\"):\n",
        "        self.gate = gate\n",
        "        self.data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
        "        self.targets = self.get_targets(gate)\n",
        "\n",
        "    def get_targets(self, gate):\n",
        "        if gate == \"AND\":\n",
        "            return torch.tensor([[0], [0], [0], [1]], dtype=torch.float32)\n",
        "        elif gate == \"OR\":\n",
        "            return torch.tensor([[0], [1], [1], [1]], dtype=torch.float32)\n",
        "        elif gate == \"XOR\":\n",
        "            return torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
        "        elif gate == \"XNOR\":\n",
        "            return torch.tensor([[1], [0], [0], [1]], dtype=torch.float32)\n",
        "\n",
        "    def step(self, input_idx, prediction):\n",
        "        target = self.targets[input_idx]\n",
        "        # Take the mean squared error\n",
        "        # print(f\"prediction: {prediction} || target: {target}\")\n",
        "        error = (prediction - target).pow(2).mean().item()\n",
        "        reward = 1.0 - error\n",
        "        return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vRGKeCcJUWLU"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "def train_logic_gate(gate=\"XOR\", epochs=10, learning_rate=0.001, batch_size=64, k_epochs=4, epsilon=0.2):\n",
        "    print(f\"Training {gate} gate with {epochs} epochs, {learning_rate} learning rate, and {batch_size} batch size.\")\n",
        "    env = LogicGateEnv(gate)\n",
        "\n",
        "    Policy_New = LogicNet()\n",
        "\n",
        "    optimizer = optim.Adam(Policy_New.parameters(), lr=learning_rate)\n",
        "\n",
        "    num_correct = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        #START OF ADVANTAGE CALCULATION\n",
        "        #global stack\n",
        "        rewards_batch = []\n",
        "        inputs_batch = []\n",
        "        targets_batch = []\n",
        "\n",
        "        # --- 1. Collect a Batch of Experiences ---\n",
        "        # Loop agent prediction, push reward value:\n",
        "        for i in range(batch_size):\n",
        "            # Get model inputs and target\n",
        "            idx = random.randint(0, 3)\n",
        "            inputs = env.data[idx]\n",
        "            target = env.targets[idx]\n",
        "\n",
        "            # Get model prediction\n",
        "            # Get logits from current policy to make a prediction for reward calculation\n",
        "            with torch.no_grad(): # No need to track gradients during data collection\n",
        "                prediction_logits = Policy_New(inputs)\n",
        "                pred = torch.round(torch.sigmoid(prediction_logits)).float() # (might need .item())\n",
        "\n",
        "            # Calculate reward\n",
        "            reward = env.step(idx, pred)\n",
        "\n",
        "            # Append to lists\n",
        "            inputs_batch.append(inputs)\n",
        "            rewards_batch.append(reward)\n",
        "            targets_batch.append(target)\n",
        "\n",
        "        # Convert collected batch lists into PyTorch tensors\n",
        "        inputs_batch_tensor = torch.stack(inputs_batch)\n",
        "        targets_batch_tensor = torch.stack(targets_batch)\n",
        "        rewards_batch_tensor = torch.tensor(rewards_batch, dtype=torch.float32)\n",
        "\n",
        "        num_correct += (rewards_batch_tensor).sum().item()  ### need to change\n",
        "\n",
        "        # Unsqueeze to ensure rewards_batch_t has the same shape as targets_batch_t for element-wise ops SHAPE:(1, batch_size)\n",
        "        rewards_batch_t = rewards_batch_tensor.unsqueeze(1)\n",
        "\n",
        "        # --- START OF ADVANTAGE CALCULATION ---\n",
        "        # Calculate the mean of the rewards in the current batch\n",
        "        mean_reward = rewards_batch_tensor.mean()\n",
        "\n",
        "        # Calculate the standard deviation of the rewards in the current batch\n",
        "        # Add a small epsilon (1e-8) to prevent division by zero in case all rewards are identical\n",
        "        std_reward = rewards_batch_tensor.std() + 1e-8\n",
        "\n",
        "        # Calculate the advantage for each time step in the batch using your specified formula\n",
        "        advantages_of_batch = (rewards_batch_t - mean_reward) / (std_reward)\n",
        "\n",
        "        # --- END OF ADVANTAGE CALCULATION ---\n",
        "\n",
        "        # --- 2. Store \"Old Policy\" Parameters ---\n",
        "        # Transfer the weights to the Old Policy Model\n",
        "        Policy_Old = LogicNet()\n",
        "        Policy_Old.load_state_dict(Policy_New.state_dict())\n",
        "        Policy_Old.eval()       # Tells Pytorch not to calculate gradients for this network\n",
        "\n",
        "        # Get log_probabilities for the collected 'targets' from the OLD policy\n",
        "        # Detach these to prevent gradients from flowing back into old_net\n",
        "        with torch.no_grad():\n",
        "            old_logits = Policy_Old(inputs_batch_tensor)\n",
        "            # Use the get_action_and_or_log_prob helper\n",
        "            log_prob_old = Policy_Old.get_action_and_or_log_prob(inputs_batch_tensor, targets_batch_tensor).detach()\n",
        "            # The .detach() is critical here to ensure old_net remains fixed.\n",
        "\n",
        "        # --- 3. Inner Optimization Loop (K_epochs) --- GRPO iteration\n",
        "        for _ in range(k_epochs):\n",
        "            new_policy_logits = Policy_New(inputs_batch_tensor)\n",
        "            log_prob_new = Policy_New.get_action_and_or_log_prob(inputs_batch_tensor, targets_batch_tensor)\n",
        "\n",
        "            # print(f\"log_prob_new: {log_prob_new}\")\n",
        "            # print(f\"log_prob_old: {log_prob_old}\")\n",
        "\n",
        "            # Calculate the ratio of each Trajectory in the Group\n",
        "            # r_t(0) = π_0(a_t|s_t) / π_0_old(a_t|s_t) = exp(log(π_0(a_t|s_t) - log(π_0_old(a_t|s_t)))\n",
        "            ratio = torch.exp(log_prob_new - log_prob_old)\n",
        "\n",
        "            surrogate_1 = ratio * advantages_of_batch\n",
        "            surrogate_2 = torch.clamp(input=ratio, min = 1.0 - epsilon, max = 1.0 + epsilon) * advantages_of_batch\n",
        "\n",
        "            loss = -torch.min(surrogate_1, surrogate_2).mean()\n",
        "            # In GRPO, the objective function is typically designed to be maximized (e.g., maximizing the expected return). Since PyTorch optimizers are designed for minimization, the common practice is to minimize the negative of the objective function.\n",
        "\n",
        "            # Update the New Policy Model\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        #epoch count\n",
        "        if epoch % 1000 == 0:\n",
        "            avg_reward = rewards_batch_tensor.mean().item()\n",
        "            print(f\"Epoch {epoch}: Loss = {loss.item()}, Avg Reward = {avg_reward:.4f}, Mean Advantage: {advantages_of_batch.mean().item()}\")\n",
        "            # Validate\n",
        "            with torch.no_grad():\n",
        "                for i in range(4):\n",
        "                    logits = Policy_New(env.data[i])\n",
        "                    pred = torch.round(torch.sigmoid(logits)).item()\n",
        "                    print(f\"Input: {env.data[i].tolist()}, Logits: {logits}, Prediction: {pred}, Actual: {env.targets[i].item()}\")\n",
        "    print(\"Training completed.\\n\")\n",
        "    print(f\"Number of correct predictions: {num_correct}/{epochs * batch_size}\")\n",
        "    print(f\"Accuracy: {num_correct/(epochs * batch_size)}\",)\n",
        "\n",
        "    print(\"\\nTesting model:\")\n",
        "    for i in range(4):\n",
        "        logits = Policy_New(env.data[i])\n",
        "        pred = torch.round(torch.sigmoid(logits)).item()\n",
        "        print(f\"Input: {env.data[i].tolist()}, Prediction: {pred}, Actual: {env.targets[i].item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVsyQrc6N31_",
        "outputId": "7f616478-cc68-419e-90c7-2234d3bd2028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training AND gate with 10 epochs, 0.001 learning rate, and 64 batch size.\n",
            "Epoch 0: Loss = -0.001682821661233902, Avg Reward = 0.6719, Mean Advantage: -3.725290298461914e-09\n",
            "Input: [0.0, 0.0], Logits: tensor([-0.1018]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([-0.0248]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 0.0], Logits: tensor([-0.0962]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 1.0], Logits: tensor([-0.0962]), Prediction: 0.0, Actual: 1.0\n",
            "Training completed.\n",
            "\n",
            "Number of correct predictions: 485.0/640\n",
            "Accuracy: 0.7578125\n",
            "\n",
            "Testing model:\n",
            "Input: [0.0, 0.0], Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 0.0], Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 1.0], Prediction: 0.0, Actual: 1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Run training\n",
        "train_logic_gate(\"AND\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "first_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
