{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg4EjgNMYmSn"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "PPO Loss\n",
        "\n",
        "* Terrrible loss, very one sided predictions\n",
        "* Needs Actor to provide rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K8F-BVwxNvrC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D4bl4-xNwvl"
      },
      "outputs": [],
      "source": [
        "# Define the neural network\n",
        "class LogicNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogicNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 4)  # Input layer -> Hidden Layer\n",
        "        self.fc2 = nn.Linear(4, 1)  # Hidden Layer -> Output Layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        logits = self.fc2(x)\n",
        "        return logits\n",
        "\n",
        "    def get_action_and_or_log_prob(self, state, action=None):\n",
        "        \"\"\"Helper method to get action and its log_prob from logits\"\"\"\n",
        "        logits = self.forward(state)    # Get the logits from a forward pass of the Policy Network\n",
        "        # For a binary output (0 or 1), Bernoulli distribution is appropriate\n",
        "        probs = torch.distributions.Bernoulli(logits=logits)\n",
        "\n",
        "        if action is None:\n",
        "            sampled_action = probs.sample() # Sample action based on current probabilities (returns 0 or 1)\n",
        "            log_prob = probs.log_prob(sampled_action)   # Calculate the log of the probability that the sampled action is chosen\n",
        "            return sampled_action, log_prob\n",
        "        else:\n",
        "            log_prob = probs.log_prob(action)       # returns the log of the probability the action is chosen\n",
        "            return log_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "50L6eW4tNxrC"
      },
      "outputs": [],
      "source": [
        "# Define the environment\n",
        "class LogicGateEnv:\n",
        "    def __init__(self, gate=\"AND\"):\n",
        "        self.gate = gate\n",
        "        self.data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
        "        self.targets = self.get_targets(gate)\n",
        "\n",
        "    def get_targets(self, gate):\n",
        "        if gate == \"AND\":\n",
        "            return torch.tensor([[0], [0], [0], [1]], dtype=torch.float32)\n",
        "        elif gate == \"OR\":\n",
        "            return torch.tensor([[0], [1], [1], [1]], dtype=torch.float32)\n",
        "        elif gate == \"XOR\":\n",
        "            return torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
        "        elif gate == \"XNOR\":\n",
        "            return torch.tensor([[1], [0], [0], [1]], dtype=torch.float32)\n",
        "\n",
        "    def step(self, input_idx, prediction):\n",
        "        target = self.targets[input_idx]\n",
        "        # Take the mean squared error\n",
        "        # print(f\"prediction: {prediction} || target: {target}\")\n",
        "        error = (prediction - target).pow(2).mean().item()\n",
        "        reward = 1.0 - error\n",
        "        return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRGKeCcJUWLU"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "def train_logic_gate(gate=\"XOR\", epochs=10, learning_rate=0.001, batch_size=64, k_epochs=4, epsilon=0.2):\n",
        "    print(f\"Training {gate} gate with {epochs} epochs, {learning_rate} learning rate, and {batch_size} batch size.\")\n",
        "    env = LogicGateEnv(gate)\n",
        "\n",
        "    Policy_New = LogicNet()   # STEP 1 || CREATE π_new\n",
        "\n",
        "    optimizer = optim.Adam(Policy_New.parameters(), lr=learning_rate)\n",
        "\n",
        "    overall_num_correct = 0.0\n",
        "    # STEP 2 || FOR I ITERATION STEPS OMITTED\n",
        "    # STEP 3 || CREATE REFERENCE MODEL OMITTED\n",
        "    for epoch in range(epochs):     # STEP 4 || FOR M ITERATION STEPS\n",
        "\n",
        "        # Initialize global stack\n",
        "        rewards_batch = []  # will be a list of floats\n",
        "        inputs_batch = []   # will be a list of tensors\n",
        "        targets_batch = []  # will be a list of tensors\n",
        "        log_prob_batch =[]  # will be a list of tensors\n",
        "\n",
        "        # STEP 5 || Sample a batch D_b from D --> OMITTED \n",
        "        # STEP 6 || Update the old policy model π_old <- π_new\n",
        "        Policy_Old = LogicNet()\n",
        "        Policy_Old.load_state_dict(Policy_New.state_dict())\n",
        "        Policy_Old.eval()   # Prevent Gradient tracking\n",
        "        \n",
        "        # --- STEP 7 || Collect a Batch of Experiences ---\n",
        "        # Gather Old Agent's predictions, push inputs, reward, and targets to history:\n",
        "        for i in range(batch_size):\n",
        "            # Get model inputs and target\n",
        "            idx = random.randint(0, 3)\n",
        "            inputs = env.data[idx]\n",
        "            target = env.targets[idx]\n",
        "\n",
        "            # Get model prediction and log_prob of that prediction\n",
        "            with torch.no_grad(): # No need to track gradients during data collection\n",
        "                pred, log_prob  = Policy_Old.get_action_and_or_log_prob(state=inputs)   # returns tensors\n",
        "\n",
        "            # Calculate reward\n",
        "            reward = env.step(idx, pred.item())\n",
        "\n",
        "            # Append to lists\n",
        "            inputs_batch.append(inputs)\n",
        "            rewards_batch.append(reward)\n",
        "            targets_batch.append(target)\n",
        "            log_prob_batch.append(log_prob)\n",
        "\n",
        "\n",
        "        # Convert collected batch lists into PyTorch tensors\n",
        "        inputs_batch_tensor = torch.stack(inputs_batch)     # Shape: (batch_size, 3)\n",
        "        targets_old_batch_tensor = torch.stack(targets_batch)   # Shape: (batch_size, 1)\n",
        "        rewards_batch_tensor = torch.tensor(rewards_batch, dtype=torch.float32)     # Shape: (batch_size,)\n",
        "        log_prob_old_batch_tensor = torch.stack(log_prob_batch)   # Shape: (batch_size, 1)\n",
        "\n",
        "        overall_num_correct += (rewards_batch_tensor).sum().item() # may need to change this calculation\n",
        "\n",
        "        # STEP 8 || Calculate Discounted Rewards\n",
        "        # Unsqueeze to ensure rewards_batch_t has the same shape as targets_batch_t for element-wise ops SHAPE:(1, batch_size)\n",
        "        rewards_batch_t = rewards_batch_tensor.unsqueeze(1)\n",
        "\n",
        "        # --- STEP 9 || START OF ADVANTAGE CALCULATION ---\n",
        "        # Calculate the mean of the rewards in the current batch\n",
        "        mean_reward = rewards_batch_tensor.mean()\n",
        "\n",
        "        # Calculate the standard deviation of the rewards in the current batch\n",
        "        # Add a small epsilon (1e-8) to prevent division by zero in case all rewards are identical\n",
        "        std_reward = rewards_batch_tensor.std() + 1e-8\n",
        "\n",
        "        # Calculate the advantage for each time step in the batch using your specified formula\n",
        "        advantages_of_batch = (rewards_batch_t - mean_reward) / (std_reward)\n",
        "\n",
        "        # --- END OF ADVANTAGE CALCULATION ---\n",
        "\n",
        "        # Detach these to prevent gradients from flowing back into old_policy_net\n",
        "        inputs_batch_tensor = inputs_batch_tensor.detach()\n",
        "        targets_old_batch_tensor = targets_old_batch_tensor.detach()\n",
        "        rewards_batch_tensor = rewards_batch_tensor.detach()\n",
        "        log_prob_old_batch_tensor = log_prob_old_batch_tensor.detach()\n",
        "\n",
        "        # --- 3. Inner Optimization Loop (K_epochs) --- GRPO iteration\n",
        "        for _ in range(k_epochs):\n",
        "            log_prob_new = Policy_New.get_action_and_or_log_prob(inputs_batch_tensor, targets_old_batch_tensor)\n",
        "\n",
        "            # print(f\"log_prob_new: {log_prob_new}\")\n",
        "            # print(f\"log_prob_old: {log_prob_old}\")\n",
        "\n",
        "            # Calculate the ratio of each Trajectory in the Group\n",
        "            # r_t(0) = π_0(a_t|s_t) / π_0_old(a_t|s_t) = exp(log(π_0(a_t|s_t) - log(π_0_old(a_t|s_t)))\n",
        "            ratio = torch.exp(log_prob_new - log_prob_old_batch_tensor)\n",
        "\n",
        "            surrogate_1 = ratio * advantages_of_batch\n",
        "            surrogate_2 = torch.clamp(input=ratio, min = 1.0 - epsilon, max = 1.0 + epsilon) * advantages_of_batch\n",
        "\n",
        "            loss = -torch.min(surrogate_1, surrogate_2).mean()\n",
        "            # In GRPO, the objective function is typically designed to be maximized (e.g., maximizing the expected return). Since PyTorch optimizers are designed for minimization, the common practice is to minimize the negative of the objective function.\n",
        "\n",
        "            # Update the New Policy Model\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Logging Metrics\n",
        "        if epoch % 1000 == 0:\n",
        "            avg_reward = rewards_batch_tensor.mean().item()\n",
        "            print(f\"Epoch {epoch}: Loss = {loss.item()}, Avg Reward = {avg_reward:.4f}, Mean Advantage: {advantages_of_batch.mean().item()}\")\n",
        "            # Validate\n",
        "            with torch.no_grad():\n",
        "                for i in range(4):\n",
        "                    logits = Policy_New(env.data[i])\n",
        "                    pred = torch.round(torch.sigmoid(logits)).item()    # Use Greedy Policy\n",
        "                    print(f\"Input: {env.data[i].tolist()}, Logits: {logits}, Prediction: {pred}, Actual: {env.targets[i].item()}\")\n",
        "    print(\"Training completed.\\n\")\n",
        "    print(f\"Number of correct predictions: {overall_num_correct}/{epochs * batch_size}\")\n",
        "    print(f\"Accuracy: {overall_num_correct/(epochs * batch_size)}\",)\n",
        "\n",
        "    print(\"\\nTesting model:\")\n",
        "    for i in range(4):\n",
        "        logits = Policy_New(env.data[i])\n",
        "        pred = torch.round(torch.sigmoid(logits)).item()    # Use Greedy Policy\n",
        "        print(f\"Input: {env.data[i].tolist()}, Prediction: {pred}, Actual: {env.targets[i].item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVsyQrc6N31_",
        "outputId": "7f616478-cc68-419e-90c7-2234d3bd2028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training OR gate with 2000 epochs, 0.001 learning rate, and 64 batch size.\n",
            "Epoch 0: Loss = -0.05957331135869026, Avg Reward = 0.5312, Mean Advantage: -1.4901161193847656e-08\n",
            "Input: [0.0, 0.0], Logits: tensor([-0.3166]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([-0.4693]), Prediction: 0.0, Actual: 1.0\n",
            "Input: [1.0, 0.0], Logits: tensor([-0.1212]), Prediction: 0.0, Actual: 1.0\n",
            "Input: [1.0, 1.0], Logits: tensor([-0.4014]), Prediction: 0.0, Actual: 1.0\n",
            "Epoch 1000: Loss = -0.01887717843055725, Avg Reward = 0.5781, Mean Advantage: -7.450580596923828e-09\n",
            "Input: [0.0, 0.0], Logits: tensor([-0.1293]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([-0.1401]), Prediction: 0.0, Actual: 1.0\n",
            "Input: [1.0, 0.0], Logits: tensor([-0.0148]), Prediction: 0.0, Actual: 1.0\n",
            "Input: [1.0, 1.0], Logits: tensor([-0.0333]), Prediction: 0.0, Actual: 1.0\n",
            "Training completed.\n",
            "\n",
            "Number of correct predictions: 63902.0/128000\n",
            "Accuracy: 0.499234375\n",
            "\n",
            "Testing model:\n",
            "Input: [0.0, 0.0], Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Prediction: 0.0, Actual: 1.0\n",
            "Input: [1.0, 0.0], Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 1.0], Prediction: 0.0, Actual: 1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Run training\n",
        "train_logic_gate(gate=\"OR\", epochs=2000, learning_rate=0.001, batch_size=64, k_epochs=4, epsilon=0.2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "GRPO_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
