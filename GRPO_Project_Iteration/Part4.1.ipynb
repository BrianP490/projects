{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg4EjgNMYmSn"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "GRPO Loss\n",
        "\n",
        "Improvements:\n",
        "* Added Log Counter as train function parameter\n",
        "\n",
        "Attempted the Following:\n",
        "* Adding Gradient Clipping\n",
        "* Tried adjusting the reward logic\n",
        "* Added Entropy penalty to Loss calculation\n",
        "\n",
        "Look into:\n",
        "* detatching advantages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "K8F-BVwxNvrC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4D4bl4-xNwvl"
      },
      "outputs": [],
      "source": [
        "# Define the neural network\n",
        "class LogicNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogicNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 4)  # Input layer -> Hidden Layer\n",
        "        self.fc2 = nn.Linear(4, 1)  # Hidden Layer -> Output Layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        logits = self.fc2(x)\n",
        "        return logits\n",
        "\n",
        "    def get_action_and_or_log_prob(self, state, action=None):\n",
        "        \"\"\"Helper method to get action and its log_prob from logits\"\"\"\n",
        "        logits = self.forward(state)    # Get the logits from a forward pass of the Policy Network\n",
        "        # For a binary output (0 or 1), Bernoulli distribution is appropriate\n",
        "        probs = torch.distributions.Bernoulli(logits=logits)\n",
        "\n",
        "        if action is None:\n",
        "            sampled_action = probs.sample() # Sample action based on current probabilities (returns 0 or 1)\n",
        "            log_prob = probs.log_prob(sampled_action)   # Calculate the log of the probability the sampled action is chosen\n",
        "            return sampled_action, log_prob\n",
        "        else:\n",
        "            log_prob = probs.log_prob(action)       # Returns the log of the probability the action is chosen\n",
        "            return log_prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "50L6eW4tNxrC"
      },
      "outputs": [],
      "source": [
        "# Define the environment\n",
        "class LogicGateEnv:\n",
        "    def __init__(self, gate=\"AND\"):\n",
        "        self.gate = gate\n",
        "        self.data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
        "        self.targets = self.get_targets(gate)\n",
        "\n",
        "    def get_targets(self, gate:str):\n",
        "        if gate == \"AND\":\n",
        "            return torch.tensor([[0], [0], [0], [1]], dtype=torch.float32)\n",
        "        elif gate == \"OR\":\n",
        "            return torch.tensor([[0], [1], [1], [1]], dtype=torch.float32)\n",
        "        elif gate == \"XOR\":\n",
        "            return torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
        "        elif gate == \"XNOR\":\n",
        "            return torch.tensor([[1], [0], [0], [1]], dtype=torch.float32)\n",
        "\n",
        "    def step(self, input_idx: int, prediction):\n",
        "        target = self.targets[input_idx].item()\n",
        "        # Rounds up to 1 if it is >=.5 to get prediction; else 0\n",
        "        reward = 1.0 if round(prediction.item()) == target else -10.0\n",
        "        return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRGKeCcJUWLU"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "def train_logic_gate(gate=\"XOR\", epochs=100, learning_rate=0.0001, batch_size=64, k_epochs=64, epsilon=0.2, beta_kl=0.01, max_grad_norm=0.5, entropy_coeff=0.5, log_iterations=10):\n",
        "    print(f\"Training {gate} gate with {epochs} epochs, {learning_rate} learning rate, batch size {batch_size}, and KL beta {beta_kl}.\")\n",
        "    #Initialize Agent's Policy, Environment, parameter optimizer, and Total Correct Counter\n",
        "    env = LogicGateEnv(gate)\n",
        "    Policy_New = LogicNet()\n",
        "    optimizer = optim.Adam(Policy_New.parameters(), lr=learning_rate)\n",
        "    num_correct = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        rewards_batch = []\n",
        "        inputs_batch = []\n",
        "        targets_batch = []\n",
        "\n",
        "        # --- 1. Collect a Batch of Experiences ---\n",
        "        # Loop agent prediction, recording important values to lists:\n",
        "        for i in range(batch_size):\n",
        "            # Get model inputs and target\n",
        "            idx = random.randint(0, 3)\n",
        "            inputs = env.data[idx]\n",
        "            target = env.targets[idx]\n",
        "\n",
        "            # Get model prediction\n",
        "            # Get logits from current policy and formulate the model's prediction for reward calculation\n",
        "            with torch.no_grad(): # No need to track gradients during data collection\n",
        "                prediction_logits = Policy_New(inputs)\n",
        "                # print(f\"prediction logits: {prediction_logits}\")\n",
        "                pred = torch.round(torch.sigmoid(prediction_logits)).float()\n",
        "\n",
        "            # Calculate reward\n",
        "            reward = env.step(idx, pred)\n",
        "\n",
        "            # Append to lists\n",
        "            inputs_batch.append(inputs)\n",
        "            rewards_batch.append(reward)\n",
        "            targets_batch.append(target)\n",
        "\n",
        "        # Convert collected batch lists into PyTorch tensors\n",
        "        inputs_batch_tensor = torch.stack(inputs_batch)\n",
        "        targets_batch_tensor = torch.stack(targets_batch)\n",
        "        rewards_batch_tensor = torch.tensor(rewards_batch, dtype=torch.float32)\n",
        "\n",
        "        num_correct += (rewards_batch_tensor==1.0).sum().item()  ### need to change\n",
        "        # print(f\"Number correct, this iteration: {(rewards_batch_tensor).sum().item()}\")\n",
        "\n",
        "        # Unsqueeze to ensure rewards_batch_t has the same shape as targets_batch_t for element-wise ops SHAPE:(1, batch_size)\n",
        "        rewards_batch_t = rewards_batch_tensor.unsqueeze(1)\n",
        "\n",
        "        # --- START OF ADVANTAGE CALCULATION ---\n",
        "        # Calculate the mean of the rewards in the current batch\n",
        "        mean_reward = rewards_batch_tensor.mean()\n",
        "\n",
        "        # Calculate the standard deviation of the rewards in the current batch\n",
        "        # Add a small epsilon (1e-8) to prevent division by zero in case all rewards are identical\n",
        "        std_reward = rewards_batch_tensor.std() + 1e-8\n",
        "\n",
        "        # print(f\"rewards_batch_t shape: {rewards_batch_t.shape} || mean_reward: {mean_reward}\")\n",
        "        # Calculate the advantage for each time step in the batch using your specified formula\n",
        "        advantages_of_batch = (rewards_batch_t - mean_reward) / (std_reward)\n",
        "        # --- END OF ADVANTAGE CALCULATION ---\n",
        "\n",
        "        # --- 2. Store \"Old Policy\" Parameters ---\n",
        "        # Transfer the weights to the Old Policy Model\n",
        "        Policy_Old = LogicNet()\n",
        "        Policy_Old.load_state_dict(Policy_New.state_dict())\n",
        "        Policy_Old.eval()       # Tells Pytorch not to calculate gradients for this network\n",
        "\n",
        "        # Get log_probabilities for the collected 'targets' from the OLD policy\n",
        "        # Detach these to prevent gradients from flowing back into old_net\n",
        "        with torch.no_grad():\n",
        "            old_logits = Policy_Old(inputs_batch_tensor)\n",
        "            # Use the get_action_and_or_log_prob helper\n",
        "            log_prob_old = Policy_Old.get_action_and_or_log_prob(inputs_batch_tensor, targets_batch_tensor).detach()\n",
        "            # The .detach() is critical here to ensure old_net remains fixed.\n",
        "\n",
        "        # --- 3. Inner Loop (K_epochs) --- GRPO Optimization iteration\n",
        "        for _ in tqdm(range(k_epochs), desc=f\"Epoch {epoch+1}/{epochs} (Inner K-Epochs)\", leave=False):\n",
        "            new_policy_logits = Policy_New(inputs_batch_tensor)\n",
        "            log_prob_new = Policy_New.get_action_and_or_log_prob(inputs_batch_tensor, targets_batch_tensor)\n",
        "\n",
        "\n",
        "            # --- KL Divergence Calculation ---\n",
        "            # Create Bernoulli distributions for new and old policies using their logits\n",
        "            p_dist = torch.distributions.Bernoulli(logits=new_policy_logits)\n",
        "            q_dist = torch.distributions.Bernoulli(logits=old_logits) # Use the detached old_logits\n",
        "\n",
        "            # Calculate KL divergence per sample, then take the mean over the batch\n",
        "            kl_div_per_sample = torch.distributions.kl.kl_divergence(p_dist, q_dist)\n",
        "            kl_loss = kl_div_per_sample.mean() # Mean over the batch\n",
        "\n",
        "\n",
        "            # print(f\"log_prob_new: {log_prob_new}\")\n",
        "            # print(f\"log_prob_old: {log_prob_old}\")\n",
        "\n",
        "            # Calculate the ratio of each Trajectory in the Group\n",
        "            # r_t(0) = π_0(a_t|s_t) / π_0_old(a_t|s_t) = exp(log(π_0(a_t|s_t) - log(π_0_old(a_t|s_t)))\n",
        "            ratio = torch.exp(log_prob_new - log_prob_old)\n",
        "\n",
        "            # print(f\"Ratio: {ratio}\")\n",
        "\n",
        "            surrogate_1 = ratio * advantages_of_batch\n",
        "            surrogate_2 = torch.clamp(input=ratio, min= 1.0 - epsilon, max= 1.0 + epsilon) * advantages_of_batch\n",
        "\n",
        "            # Combine clipped loss with KL penalty\n",
        "            # Remember: we minimize the negative of the main objective, and add the KL term\n",
        "            # Maximize: min(...) - beta * D_KL(...) => Minimize: -min(...) + beta * D_KL(...)\n",
        "            policy_objective_term = -torch.min(surrogate_1, surrogate_2).mean()\n",
        "\n",
        "            # print(f\"policy_objective_term: {policy_objective_term}\")\n",
        "            \n",
        "            # Calculate the entropy \n",
        "            entropy = p_dist.entropy().mean()\n",
        "            # print(f\"Entropy Monitor: {entropy_coeff * entropy}\") # Entropy goes to 0\n",
        "            loss = policy_objective_term + beta_kl * kl_loss - entropy_coeff * entropy# Add KL term with beta_kl weight\n",
        "\n",
        "            # In GRPO, the objective function is typically designed to be maximized (e.g., maximizing the expected return). Since PyTorch optimizers are designed for minimization, the common practice is to minimize the negative of the objective function.\n",
        "\n",
        "            # Update the New Policy Model\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # --- ADDING GRADIENT CLIPPING HERE TO LIMIT PARAMETER UPDATES---\n",
        "            # torch.nn.utils.clip_grad_norm_(Policy_New.parameters(), max_norm=max_grad_norm)\n",
        "            # ----------------------------------\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        # LOG IF ENOUGH EPOCHS HAVE ELAPSED\n",
        "        if epoch % log_iterations == 0:\n",
        "            avg_reward = rewards_batch_tensor.mean().item()\n",
        "            print(f\"Epoch {epoch}: Loss = {loss.item()}, Avg Reward = {avg_reward:.4f}, Mean Advantage: {advantages_of_batch.mean().item()}\")\n",
        "            # Validation Step\n",
        "            print(\"Validating the Model:\")\n",
        "            with torch.no_grad():\n",
        "                for i in range(4):\n",
        "                    logits = Policy_New(env.data[i])\n",
        "                    pred = torch.round(torch.sigmoid(logits)).item()\n",
        "                    print(f\"Input: {env.data[i].tolist()}, Logits: {logits}, Prediction: {pred}, Actual: {env.targets[i].item()}\")\n",
        "\n",
        "    print(\"Training completed.\\n\")\n",
        "    print(f\"Number of correct predictions: {num_correct}/{epochs * batch_size}\")\n",
        "    print(f\"Accuracy: {num_correct/(epochs * batch_size)}%\")\n",
        "\n",
        "    print(\"\\nTesting Trained Model:\")\n",
        "    for i in range(4):\n",
        "        logits = Policy_New(env.data[i])\n",
        "        pred = torch.round(torch.sigmoid(logits)).item()\n",
        "        print(f\"Input: {env.data[i].tolist()}, Prediction: {pred}, Actual: {env.targets[i].item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "OVsyQrc6N31_",
        "outputId": "7f992305-8ace-4a9e-f1fd-272462e5887c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training AND gate with 100 epochs, 0.0001 learning rate, batch size 64, and KL beta 0.01.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                              \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss = -0.33785897493362427, Avg Reward = -0.7188, Mean Advantage: 2.2351741790771484e-08\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([-0.4696]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([-0.4915]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 0.0], Logits: tensor([-0.4276]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 1.0], Logits: tensor([-0.4438]), Prediction: 0.0, Actual: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                              \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss = -0.33281025290489197, Avg Reward = -1.7500, Mean Advantage: 3.3527612686157227e-08\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([-0.6069]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([-0.6749]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 0.0], Logits: tensor([-0.5331]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 1.0], Logits: tensor([-0.6183]), Prediction: 0.0, Actual: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss = -0.3191086947917938, Avg Reward = -1.4062, Mean Advantage: 1.4901161193847656e-08\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([-0.7694]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([-0.9191]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 0.0], Logits: tensor([-0.6811]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 1.0], Logits: tensor([-0.8804]), Prediction: 0.0, Actual: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss = -0.2978871762752533, Avg Reward = -2.0938, Mean Advantage: -7.450580596923828e-09\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([-0.9811]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([-1.2464]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 0.0], Logits: tensor([-0.9108]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 1.0], Logits: tensor([-1.2449]), Prediction: 0.0, Actual: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40: Loss = -0.2597149610519409, Avg Reward = -2.0938, Mean Advantage: -1.1175870895385742e-08\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([-1.2173]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([-1.6331]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 0.0], Logits: tensor([-1.1935]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 1.0], Logits: tensor([-1.6909]), Prediction: 0.0, Actual: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50: Loss = -0.2265864908695221, Avg Reward = -2.6094, Mean Advantage: -7.450580596923828e-09\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([-1.4324]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([-2.0088]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 0.0], Logits: tensor([-1.5176]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 1.0], Logits: tensor([-2.1989]), Prediction: 0.0, Actual: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 60: Loss = -0.18781158328056335, Avg Reward = -2.4375, Mean Advantage: 0.0\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([-1.6692]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([-2.4700]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 0.0], Logits: tensor([-1.8971]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 1.0], Logits: tensor([-2.7892]), Prediction: 0.0, Actual: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 70: Loss = -0.14591234922409058, Avg Reward = -1.2344, Mean Advantage: -7.450580596923828e-09\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([-1.9237]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([-2.9861]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 0.0], Logits: tensor([-2.3308]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 1.0], Logits: tensor([-3.4592]), Prediction: 0.0, Actual: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 80: Loss = -0.12646956741809845, Avg Reward = -1.2344, Mean Advantage: -7.450580596923828e-09\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([-2.1854]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([-3.5261]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 0.0], Logits: tensor([-2.7926]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 1.0], Logits: tensor([-4.1695]), Prediction: 0.0, Actual: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 90: Loss = -0.098661869764328, Avg Reward = -2.2656, Mean Advantage: -2.7939677238464355e-09\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([-2.4917]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([-4.1304]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 0.0], Logits: tensor([-3.3173]), Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 1.0], Logits: tensor([-4.9723]), Prediction: 0.0, Actual: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "\n",
            "Number of correct predictions: 4808.0/6400\n",
            "Accuracy: 0.75125%\n",
            "\n",
            "Testing Trained Model:\n",
            "Input: [0.0, 0.0], Prediction: 0.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 0.0], Prediction: 0.0, Actual: 0.0\n",
            "Input: [1.0, 1.0], Prediction: 0.0, Actual: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "\n",
        "# Run training\n",
        "train_logic_gate(\"AND\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odp6rhSrlymy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "first_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
