{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg4EjgNMYmSn"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "GRPO Loss\n",
        "\n",
        "* Terrible Loss\n",
        "* Gives One Sided Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K8F-BVwxNvrC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4D4bl4-xNwvl"
      },
      "outputs": [],
      "source": [
        "# Define the neural network\n",
        "class LogicNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogicNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 4)  # Input layer -> Hidden Layer\n",
        "        self.fc2 = nn.Linear(4, 1)  # Hidden Layer -> Output Layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        logits = self.fc2(x)\n",
        "        return logits\n",
        "\n",
        "    def get_action_and_or_log_prob(self, state, action=None):\n",
        "        \"\"\"Helper method to get action and its log_prob from logits\"\"\"\n",
        "        logits = self.forward(state)    # Get the logits from a forward pass of the Policy Network\n",
        "        # For a binary output (0 or 1), Bernoulli distribution is appropriate\n",
        "        probs = torch.distributions.Bernoulli(logits=logits)\n",
        "\n",
        "        if action is None:\n",
        "            sampled_action = probs.sample() # Sample action based on current probabilities (returns 0 or 1)\n",
        "            log_prob = probs.log_prob(sampled_action)   # Calculate the log of the probability the sampled action is chosen\n",
        "            return sampled_action, log_prob\n",
        "        else:\n",
        "            log_prob = probs.log_prob(action)       # Returns the log of the probability the action is chosen\n",
        "            return log_prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "50L6eW4tNxrC"
      },
      "outputs": [],
      "source": [
        "# Define the environment\n",
        "class LogicGateEnv:\n",
        "    def __init__(self, gate=\"AND\"):\n",
        "        self.gate = gate\n",
        "        self.data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
        "        self.targets = self.get_targets(gate)\n",
        "\n",
        "    def get_targets(self, gate:str):\n",
        "        if gate == \"AND\":\n",
        "            return torch.tensor([[0], [0], [0], [1]], dtype=torch.float32)\n",
        "        elif gate == \"OR\":\n",
        "            return torch.tensor([[0], [1], [1], [1]], dtype=torch.float32)\n",
        "        elif gate == \"XOR\":\n",
        "            return torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
        "        elif gate == \"XNOR\":\n",
        "            return torch.tensor([[1], [0], [0], [1]], dtype=torch.float32)\n",
        "\n",
        "    def step(self, input_idx: int, prediction):\n",
        "        target = self.targets[input_idx]\n",
        "        # Take the mean squared error\n",
        "        # print(f\"prediction: {prediction} || target: {target}\")\n",
        "        error = (prediction - target).pow(2).mean().item()\n",
        "        reward = 1.0 - error\n",
        "        return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRGKeCcJUWLU"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "def train_logic_gate(gate=\"XOR\", epochs=1000, learning_rate=0.0001, batch_size=64, k_epochs=64, epsilon=0.2, beta_kl=0.01, max_grad_norm=0.5):\n",
        "    print(f\"Training {gate} gate with {epochs} epochs, {learning_rate} learning rate, batch size {batch_size}, and KL beta {beta_kl}.\")\n",
        "    # Initialize Agent's Policy, Environment, parameter optimizer, and Total Correct Counter\n",
        "    env = LogicGateEnv(gate)\n",
        "    Policy_New = LogicNet()   # STEP 1 || CREATE π_new\n",
        "    optimizer = optim.Adam(Policy_New.parameters(), lr=learning_rate)\n",
        "    num_correct = 0.0\n",
        "    # STEP 2 || FOR I ITERATION STEPS OMITTED\n",
        "    # STEP 3 || CREATE REFERENCE MODEL OMITTED\n",
        "\n",
        "    for epoch in range(epochs):     # STEP 4 || FOR M ITERATION STEPS\n",
        "        rewards_batch = []  # will be a list of floats\n",
        "        inputs_batch = []   # will be a list of tensors\n",
        "        targets_batch = []  # will be a list of tensors\n",
        "        \n",
        "        # STEP 5 || Sample a batch D_b from D --> OMITTED \n",
        "        # STEP 6 || Update the old policy model π_old <- π_new\n",
        "        Policy_Old = LogicNet()\n",
        "        Policy_Old.load_state_dict(Policy_New.state_dict())\n",
        "        Policy_Old.eval()   # Prevent Gradient tracking\n",
        "\n",
        "        # --- STEP 7 || Collect a Batch of Experiences ---\n",
        "        # Loop agent prediction, recording important values to lists:\n",
        "        for i in range(batch_size):\n",
        "            # Get model inputs and target\n",
        "            idx = random.randint(0, 3)\n",
        "            inputs = env.data[idx]\n",
        "            target = env.targets[idx]\n",
        "\n",
        "            # Get model prediction and log_prob of that prediction using the old policy\n",
        "            with torch.no_grad(): # No need to track gradients during data collection\n",
        "                pred, log_prob  = Policy_Old.get_action_and_or_log_prob(state=inputs)   # returns tensors\n",
        "\n",
        "            # Calculate reward\n",
        "            reward = env.step(idx, pred.item())\n",
        "\n",
        "            # Append to lists\n",
        "            inputs_batch.append(inputs)\n",
        "            rewards_batch.append(reward)\n",
        "            targets_batch.append(target)\n",
        "\n",
        "        # Convert collected batch lists into PyTorch tensors\n",
        "        inputs_batch_tensor = torch.stack(inputs_batch)     # Shape: (batch_size, 3)\n",
        "        targets_old_batch_tensor = torch.stack(targets_batch)   # Shape: (batch_size, 1)\n",
        "        rewards_batch_tensor = torch.tensor(rewards_batch, dtype=torch.float32)     # Shape: (batch_size,)\n",
        "\n",
        "        num_correct += (rewards_batch_tensor).sum().item()  ### need to change\n",
        "        # print(f\"Number correct, this iteration: {(rewards_batch_tensor).sum().item()}\")\n",
        "\n",
        "        # STEP 8 || Calculate Discounted Rewards\n",
        "        # Unsqueeze to ensure rewards_batch_t has the same shape as targets_batch_t for element-wise ops SHAPE:(1, batch_size)\n",
        "        rewards_batch_t = rewards_batch_tensor.unsqueeze(1)\n",
        "\n",
        "        # --- STEP 9 || START OF ADVANTAGE CALCULATION ---\n",
        "        # Calculate the mean of the rewards in the current batch\n",
        "        mean_reward = rewards_batch_tensor.mean()\n",
        "\n",
        "        # Calculate the standard deviation of the rewards in the current batch\n",
        "        # Add a small epsilon (1e-8) to prevent division by zero in case all rewards are identical\n",
        "        std_reward = rewards_batch_tensor.std() + 1e-8\n",
        "        # print(f\"rewards_batch_t shape: {rewards_batch_t.shape} || mean_reward: {mean_reward}\")\n",
        "        # Calculate the advantage for each time step in the batch using your specified formula\n",
        "        advantages_of_batch = (rewards_batch_t - mean_reward) / (std_reward)\n",
        "        # --- END OF ADVANTAGE CALCULATION ---\n",
        "\n",
        "        # Detach these to prevent gradients from flowing back into old_policy_net\n",
        "        inputs_batch_tensor = inputs_batch_tensor.detach()\n",
        "        targets_old_batch_tensor = targets_old_batch_tensor.detach()\n",
        "        rewards_batch_tensor = rewards_batch_tensor.detach()\n",
        "        # log_prob_old_batch_tensor = log_prob_old_batch_tensor.detach()\n",
        "        advantages_of_batch = advantages_of_batch.detach()\n",
        "\n",
        "        # Get log_probabilities for the collected 'targets' from the OLD policy\n",
        "        # Detach these to prevent gradients from flowing back into old_net\n",
        "        with torch.no_grad():\n",
        "            old_logits = Policy_Old(inputs_batch_tensor)\n",
        "            # Use the get_action_and_or_log_prob helper\n",
        "            log_prob_old = Policy_Old.get_action_and_or_log_prob(inputs_batch_tensor, targets_old_batch_tensor).detach()\n",
        "            # The .detach() is critical here to ensure old_net remains fixed.\n",
        "            q_dist = torch.distributions.Bernoulli(logits=old_logits.detach())\n",
        "\n",
        "        # --- STEP 10 || GRPO Optimization ---\n",
        "        for _ in tqdm(range(k_epochs), desc=f\"Epoch {epoch+1}/{epochs} (Inner K-Epochs)\", leave=False):\n",
        "            new_policy_logits = Policy_New(inputs_batch_tensor)\n",
        "            log_prob_new = Policy_New.get_action_and_or_log_prob(inputs_batch_tensor, targets_old_batch_tensor)\n",
        "\n",
        "\n",
        "            # --- KL Divergence Calculation ---\n",
        "            # Create Bernoulli distributions for new and old policies using their logits\n",
        "            p_dist = torch.distributions.Bernoulli(logits=new_policy_logits)\n",
        "            # use same q_dist calculated above\n",
        "\n",
        "            # Calculate KL divergence per sample, then take the mean over the batch\n",
        "            kl_div_per_sample = torch.distributions.kl.kl_divergence(p_dist, q_dist)\n",
        "            kl_loss = kl_div_per_sample.mean() # Mean over the batch\n",
        "\n",
        "\n",
        "            # print(f\"log_prob_new: {log_prob_new}\")\n",
        "            # print(f\"log_prob_old: {log_prob_old}\")\n",
        "\n",
        "            # Calculate the ratio of each Trajectory in the Group\n",
        "            # r_t(0) = π_0(a_t|s_t) / π_0_old(a_t|s_t) = exp(log(π_0(a_t|s_t) - log(π_0_old(a_t|s_t)))\n",
        "            ratio = torch.exp(log_prob_new - log_prob_old)\n",
        "\n",
        "            # print(f\"Ratio: {ratio}\")\n",
        "\n",
        "            surrogate_1 = ratio * advantages_of_batch\n",
        "            surrogate_2 = torch.clamp(input=ratio, min= 1.0 - epsilon, max= 1.0 + epsilon) * advantages_of_batch\n",
        "\n",
        "            # Combine clipped loss with KL penalty\n",
        "            # Remember: we minimize the negative of the main objective, and add the KL term\n",
        "            # Maximize: min(...) - beta * D_KL(...) => Minimize: -min(...) + beta * D_KL(...)\n",
        "            policy_objective_term = -torch.min(surrogate_1, surrogate_2).mean()\n",
        "\n",
        "            # print(f\"policy_objective_term: {policy_objective_term}\")\n",
        "            \n",
        "            loss = policy_objective_term + beta_kl * kl_loss # Add KL term with beta_kl weight\n",
        "\n",
        "            # In GRPO, the objective function is typically designed to be maximized (e.g., maximizing the expected return). Since PyTorch optimizers are designed for minimization, the common practice is to minimize the negative of the objective function.\n",
        "\n",
        "            # STEP 11 || Policy Updates\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # --- ADDING GRADIENT CLIPPING HERE TO LIMIT PARAMETER UPDATES---\n",
        "            torch.nn.utils.clip_grad_norm_(Policy_New.parameters(), max_norm=max_grad_norm)\n",
        "            # ----------------------------------\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        # LOG IF ENOUGH EPOCHS HAVE ELAPSED\n",
        "        if epoch % 100 == 0:\n",
        "            avg_reward = rewards_batch_tensor.mean().item()\n",
        "            print(f\"Epoch {epoch}: Loss = {loss.item()}, Avg Reward = {avg_reward:.4f}, Mean Advantage: {advantages_of_batch.mean().item()}\")\n",
        "            # Validation Step\n",
        "            print(\"Validating the Model:\")\n",
        "            with torch.no_grad():\n",
        "                for i in range(4):\n",
        "                    logits = Policy_New(env.data[i])\n",
        "                    pred = torch.round(torch.sigmoid(logits)).item()\n",
        "                    print(f\"Input: {env.data[i].tolist()}, Logits: {logits}, Prediction: {pred}, Actual: {env.targets[i].item()}\")\n",
        "\n",
        "    print(\"Training completed.\\n\")\n",
        "    print(f\"Number of correct predictions: {num_correct}/{epochs * batch_size}\")\n",
        "    print(f\"Accuracy: {num_correct/(epochs * batch_size)}%\")\n",
        "\n",
        "    print(\"\\nTesting Trained Model:\")\n",
        "    for i in range(4):\n",
        "        logits = Policy_New(env.data[i])\n",
        "        pred = torch.round(torch.sigmoid(logits)).item()\n",
        "        print(f\"Input: {env.data[i].tolist()}, Prediction: {pred}, Actual: {env.targets[i].item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "OVsyQrc6N31_",
        "outputId": "7f992305-8ace-4a9e-f1fd-272462e5887c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training XOR gate with 1000 epochs, 0.0001 learning rate, batch size 64, and KL beta 0.01.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss = -0.0031273080967366695, Avg Reward = 0.4688, Mean Advantage: 1.4901161193847656e-08\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([0.2180]), Prediction: 1.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([0.4819]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 0.0], Logits: tensor([0.3433]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 1.0], Logits: tensor([0.4804]), Prediction: 1.0, Actual: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100: Loss = -0.03756774216890335, Avg Reward = 0.4062, Mean Advantage: 0.0\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([3.4598]), Prediction: 1.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([6.5537]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 0.0], Logits: tensor([6.0721]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 1.0], Logits: tensor([9.1660]), Prediction: 1.0, Actual: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 200: Loss = -0.06453421711921692, Avg Reward = 0.4375, Mean Advantage: 0.0\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([9.0213]), Prediction: 1.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([17.2350]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 0.0], Logits: tensor([16.4411]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 1.0], Logits: tensor([24.6548]), Prediction: 1.0, Actual: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 300: Loss = -0.0758737176656723, Avg Reward = 0.3438, Mean Advantage: -8.381903171539307e-09\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([17.0490]), Prediction: 1.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([32.7010]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 0.0], Logits: tensor([31.5960]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 1.0], Logits: tensor([47.2480]), Prediction: 1.0, Actual: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 400: Loss = -0.08336611837148666, Avg Reward = 0.4844, Mean Advantage: 1.5832483768463135e-08\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([27.4625]), Prediction: 1.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([52.2739]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 0.0], Logits: tensor([50.8568]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 1.0], Logits: tensor([75.6682]), Prediction: 1.0, Actual: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 500: Loss = -0.07657364010810852, Avg Reward = 0.5781, Mean Advantage: -7.450580596923828e-09\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([40.2311]), Prediction: 1.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([75.7176]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 0.0], Logits: tensor([73.9888]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 1.0], Logits: tensor([109.4752]), Prediction: 1.0, Actual: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 600: Loss = -0.08424654603004456, Avg Reward = 0.5781, Mean Advantage: -3.725290298461914e-09\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([55.3809]), Prediction: 1.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([102.9234]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 0.0], Logits: tensor([100.8833]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 1.0], Logits: tensor([148.4258]), Prediction: 1.0, Actual: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 700: Loss = -0.09047267585992813, Avg Reward = 0.4531, Mean Advantage: 3.725290298461914e-09\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([72.9494]), Prediction: 1.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([133.8081]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 0.0], Logits: tensor([131.4569]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 1.0], Logits: tensor([192.3157]), Prediction: 1.0, Actual: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 800: Loss = -0.09459985047578812, Avg Reward = 0.4062, Mean Advantage: 0.0\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([92.9401]), Prediction: 1.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([168.3655]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 0.0], Logits: tensor([165.7035]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 1.0], Logits: tensor([241.1289]), Prediction: 1.0, Actual: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 900: Loss = -0.09916718304157257, Avg Reward = 0.5156, Mean Advantage: -7.450580596923828e-09\n",
            "Validating the Model:\n",
            "Input: [0.0, 0.0], Logits: tensor([115.2267]), Prediction: 1.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Logits: tensor([206.3324]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 0.0], Logits: tensor([203.3612]), Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 1.0], Logits: tensor([294.4669]), Prediction: 1.0, Actual: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "\n",
            "Number of correct predictions: 32020.0/64000\n",
            "Accuracy: 0.5003125%\n",
            "\n",
            "Testing Trained Model:\n",
            "Input: [0.0, 0.0], Prediction: 1.0, Actual: 0.0\n",
            "Input: [0.0, 1.0], Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 0.0], Prediction: 1.0, Actual: 1.0\n",
            "Input: [1.0, 1.0], Prediction: 1.0, Actual: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "\n",
        "# Run training\n",
        "train_logic_gate(gate=\"XOR\", epochs=1000, learning_rate=0.0001, batch_size=64, k_epochs=64, epsilon=0.2, beta_kl=0.01, max_grad_norm=0.5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "GRPO_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
