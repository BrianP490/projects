{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02b061c",
   "metadata": {},
   "source": [
    "**Part 3**\n",
    "\n",
    "* Adding vectorized BlackJack Environment\n",
    "* Adding Device Agnostic code (GPU Training)\n",
    "* Attempting to use Softmax (Categorical Distribution) implementation instead of Sigmoid (Binary Bernoulli Distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c9daf",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "* Still very slow during training\n",
    "* Use CPU for training model for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d09e02",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1301783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c997ec",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83efa5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", sab=True) # `render_mode=\"human\"` creates a pygame popup window to analyze play # `sab=True` uses the Sutton & Barto version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a083c10b",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "547b9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackJackAgent(nn.Module):\n",
    "    def __init__(self, obs_size=3, hidden_size=10, output_size=2):\n",
    "        super(BlackJackAgent, self).__init__()\n",
    "        self.layer_1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.layer_2 = nn.Linear(hidden_size, output_size)\n",
    "        self.action_probs_activation_layer = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer_1(x))\n",
    "        logits = self.layer_2(x)\n",
    "        return logits       # later use nn.Softmax to get probabilities\n",
    "\n",
    "    def get_action_probs(self, logits):\n",
    "        \"\"\"Get the probabilities of each action.\"\"\"\n",
    "        return self.action_probs_activation_layer(logits)\n",
    "    \n",
    "    def sample_action(self, action:None):\n",
    "        \"\"\"Get the probability of choosing the action\"\"\"\n",
    "        logits = self.forward(action)\n",
    "        probs = self.get_action_probs(logits)\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample().item()\n",
    "        prob_of_action = dist.log_prob(action)\n",
    "        return action, prob_of_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618481e2",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_blackjack_agent(epochs=50, learning_rate=0.0001, batch_size=64, gamma=0.99, k_epochs=64, epsilon=0.2, beta_kl=0.01, max_grad_norm=0.5, entropy_coeff=0.01, log_iterations=10, device=\"cpu\", num_envs=16) -> BlackJackAgent: \n",
    "    print(f\"Training BlackJack Agent's Policy on {device} with {epochs} epochs, {learning_rate} learning rate, batch size {batch_size}, and KL beta {beta_kl}.\")\n",
    "\n",
    "    vec_env = gym.make_vec(\"Blackjack-v1\", num_envs=num_envs, sab=True) # `sab=True` uses the Sutton & Barto version\n",
    "\n",
    "    # steps_per_env_per_rollout = batch_size // num_envs if batch_size % num_envs == 0 else (batch_size // num_envs) + 1\n",
    "\n",
    "    New_Policy = BlackJackAgent().to(device)   # STEP 3 || \n",
    "    optimizer = optim.Adam(params=New_Policy.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=f\"Main Epoch (Outer Loop)\", leave=False):     # STEP 4 || \n",
    "        # STEP 5 || Sample a batch D_b from D --> OMITTED \n",
    "        # STEP 6 || Update the old policy model PI old <- PI new\n",
    "        Policy_Old = BlackJackAgent().to(device)\n",
    "        Policy_Old.load_state_dict(New_Policy.state_dict())\n",
    "        Policy_Old.eval()   # Prevent Gradient tracking\n",
    "\n",
    "        # This will store trajectories for all episodes collected in the current batch\n",
    "        completed_batch_trajectories = []\n",
    "\n",
    "        # Reset all vectorized environments\n",
    "        raw_observations, infos = vec_env.reset()  # raw_observations is a numpy array of shape (num_envs, num_envs, num_envs)\n",
    "        observations = np.stack(raw_observations, axis=1)  # Shape: (num_envs, num_envs, num_envs)  >  (num_envs, 3)\n",
    "        dones = np.array([False] * num_envs) # Track the done status for each parallel environment\n",
    "        truncateds = np.array([False] * num_envs) # Track truncated status for each parallel environment\n",
    "\n",
    "        # Initialize current trajectories for all parallel environments\n",
    "        # Each element in this list will be a dict for an *in-progress* episode in a specific env\n",
    "        current_episode_trajectories = [{\"states\": [], \"actions\": [], \"rewards\": [], \"log_probs\": []} for _ in range(num_envs)]\n",
    "\n",
    "        # --- STEP 7 Collect a Batch of Experiences Using the Old Policy---\n",
    "        # Loop Agent prediction, recording trajectories to lists:\n",
    "        episodes_collected_in_batch = 0\n",
    "        max_steps_per_batch_limit = batch_size * 5 # A safety limit to prevent infinite loops if episodes are very long\n",
    "        current_total_steps = 0\n",
    "\n",
    "        while episodes_collected_in_batch < batch_size and current_total_steps < max_steps_per_batch_limit:\n",
    "            obs_tensor = torch.tensor(observations, dtype=torch.float32).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = Policy_Old(obs_tensor)\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                actions = dist.sample()     # Tensor of shape [num_envs,]\n",
    "                log_probs = dist.log_prob(actions)     # Tensor of shape [num_envs,]\n",
    "                    \n",
    "            raw_next_obs, rewards, dones, truncateds, infos = vec_env.step(actions.cpu().numpy()) # actions must be on CPU for env.step()\n",
    "            next_obs = np.stack(raw_next_obs, axis=1)\n",
    "            current_total_steps += num_envs\n",
    "\n",
    "            # Process data for each parallel environment\n",
    "            for env_idx in range(num_envs):\n",
    "                \n",
    "                obs_to_append = observations[env_idx]\n",
    "                if isinstance(obs_to_append, torch.Tensor):\n",
    "                    obs_to_append = obs_to_append.cpu().numpy()\n",
    "                # Store current_episode_trajectories\n",
    "                current_episode_trajectories[env_idx][\"states\"].append(obs_to_append)\n",
    "                current_episode_trajectories[env_idx][\"actions\"].append(actions[env_idx].item())\n",
    "                current_episode_trajectories[env_idx][\"rewards\"].append(rewards[env_idx])\n",
    "                current_episode_trajectories[env_idx][\"log_probs\"].append(log_probs[env_idx].cpu())\n",
    "                \n",
    "                if dones[env_idx] or truncateds[env_idx]:\n",
    "                    completed_batch_trajectories.append(current_episode_trajectories[env_idx])\n",
    "                    episodes_collected_in_batch += 1\n",
    "\n",
    "                    # Reset this specific environment\n",
    "                    # new_obs, new_info = vec_env.reset_at(env_idx)\n",
    "                    # observations[env_idx] = new_obs\n",
    "\n",
    "                    current_episode_trajectories[env_idx] = {\"states\": [], \"actions\": [], \"rewards\": [], \"log_probs\": []}\n",
    "\n",
    "            observations = next_obs  # Update the observation\n",
    "\n",
    "        for env_idx in range(num_envs):\n",
    "            if len(current_episode_trajectories[env_idx][\"states\"]) > 0:\n",
    "                # If there's partial data, it means the episode was still running\n",
    "                # when `batch_size` was met. You'll need to decide how to handle this.\n",
    "                # For simplicity for now, we'll append them. In full PPO, you'd add\n",
    "                # the value of the last state to its rewards.\n",
    "                completed_batch_trajectories.append(current_episode_trajectories[env_idx])\n",
    "                # Note: These might not be \"full\" episodes in the sense of reaching a done state,\n",
    "                # but they contribute steps to your batch.\n",
    "\n",
    "        # These lists will hold data from ALL episodes in the current batch for Advantage Calculation\n",
    "        all_states = []\n",
    "        all_actions = []\n",
    "        all_old_log_probs = []\n",
    "        all_discounted_rewards = []\n",
    "\n",
    "        # STEP 8 || Calculate Discounted Rewards for completed trajectories\n",
    "        for episode_trajectory in completed_batch_trajectories: \n",
    "            rewards = episode_trajectory[\"rewards\"]\n",
    "            states = episode_trajectory[\"states\"]\n",
    "            actions = episode_trajectory[\"actions\"]\n",
    "            log_probs = episode_trajectory[\"log_probs\"]\n",
    "            \n",
    "            if not rewards:\n",
    "                continue\n",
    "\n",
    "            discounted_reward = 0\n",
    "            returns_for_episode = []\n",
    "            for reward in reversed(rewards):\n",
    "                discounted_reward = reward + gamma * discounted_reward\n",
    "                returns_for_episode.insert(0, discounted_reward)\n",
    "\n",
    "            discounted_rewards = torch.tensor(returns_for_episode, dtype=torch.float32) # turn list of rewards shape (trajectory_len ) to a Tensor of shape (trajectory_len, )\n",
    "            # print(f\"discounted_rewards size: {discounted_rewards.size()}\")\n",
    "            # Add each trajectory information for the batch\n",
    "            if states:\n",
    "                all_states.extend(states)\n",
    "                all_actions.extend(actions)\n",
    "                all_old_log_probs.extend(log_probs)\n",
    "                all_discounted_rewards.extend(discounted_rewards.tolist())\n",
    "\n",
    "        # --- IMPORTANT: Pre-tensorization checks and conversions ---\n",
    "        if not all_states or not all_actions or not all_old_log_probs or not all_discounted_rewards:\n",
    "            print(f\"Warning: Epoch {epoch + 1}: Insufficient data collected for optimization. \"\n",
    "                  f\"Skipping policy update for this epoch.\")\n",
    "            print(f\"  Counts: States={len(all_states)}, Actions={len(all_actions)}, \"\n",
    "                  f\"LogProbs={len(all_old_log_probs)}, Rewards={len(all_discounted_rewards)}\")\n",
    "            continue\n",
    "        # Convert all collected batch data into PyTorch tensors\n",
    "        all_states_tensor = torch.tensor(np.array(all_states), dtype=torch.float32).to(device)\n",
    "        all_actions_tensor = torch.tensor(all_actions, dtype=torch.long).to(device)\n",
    "        # Stack individual log_prob tensors and then flatten if necessary\n",
    "        all_old_log_probs_tensor = torch.tensor(all_old_log_probs, dtype=torch.float32).to(device) # Ensure it's a 1D tensor\n",
    "        all_discounted_rewards_tensor = torch.tensor(all_discounted_rewards, dtype=torch.float32).to(device)\n",
    "\n",
    "        # STEP 9 || Calculate the Advantage of each Time Step for each Trajectory using normalization\n",
    "        all_advantages_tensor = (all_discounted_rewards_tensor - all_discounted_rewards_tensor.mean()) / (all_discounted_rewards_tensor.std() + 1e-8)\n",
    "\n",
    "        # Detach these tensors from any computation graph history\n",
    "        # as they represent fixed data for the policy updates in k_epochs.\n",
    "        # This prevents the \"RuntimeError: Trying to backward through the graph a second time\".\n",
    "        all_states_tensor = all_states_tensor.detach()\n",
    "        all_actions_tensor = all_actions_tensor.detach()\n",
    "        all_old_log_probs_tensor = all_old_log_probs_tensor.detach()\n",
    "        all_advantages_tensor = all_advantages_tensor.detach()\n",
    "\n",
    "        # Create distributions for old policy for use in KL Divergence calculation later \n",
    "        with torch.no_grad():\n",
    "            old_logits = Policy_Old(all_states_tensor)\n",
    "            old_dist = torch.distributions.Categorical(logits=old_logits)\n",
    "\n",
    "        New_Policy.train()  # Prepare NN for updates\n",
    "\n",
    "        # --- STEP 10 || GRPO Optimization ---\n",
    "        for k_epoch in tqdm(range(k_epochs), desc=f\"Epoch {epoch+1}/{epochs} (Inner K-Epochs)\", leave=True):\n",
    "            new_logits = New_Policy(all_states_tensor)\n",
    "            new_dist = torch.distributions.Categorical(logits=new_logits)\n",
    "            new_log_probs = new_dist.log_prob(all_actions_tensor)\n",
    "            entropy = new_dist.entropy().mean() # Calculate entropy for regularization\n",
    "\n",
    "            R1_ratio = torch.exp(new_log_probs - all_old_log_probs_tensor)\n",
    "\n",
    "            unclipped_surrogate = R1_ratio * all_advantages_tensor\n",
    "            clipped_surrogate = torch.clamp(input=R1_ratio, min=1.0-epsilon, max=1.0+epsilon) * all_advantages_tensor\n",
    "\n",
    "            policy_loss = -torch.min(unclipped_surrogate, clipped_surrogate).mean()\n",
    "\n",
    "            # --- KL Divergence Calculation ---\n",
    "            # Calculate KL divergence per sample, then take the mean over the batch\n",
    "            kl_div_per_sample = torch.distributions.kl.kl_divergence(p=new_dist, q=old_dist)\n",
    "            kl_loss = kl_div_per_sample.mean() # Mean over the batch\n",
    "\n",
    "            # Total Loss for GRPO\n",
    "            total_loss = policy_loss + beta_kl * kl_loss - entropy_coeff * entropy\n",
    "\n",
    "            # STEP 11 || Policy Updates\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(New_Policy.parameters(), max_grad_norm)\n",
    "            optimizer.step()    # Update policy parameters using gradient ascent\n",
    "        \n",
    "        \n",
    "        # --- 4. Logging and Evaluation ---\n",
    "        if (epoch + 1) % log_iterations == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}, Ratio: {R1_ratio.mean().item():.5f}, Entropy Term: {entropy:.5f}\")\n",
    "            # You can add more evaluation metrics here, e.g., average reward per episode\n",
    "            # For Blackjack, the reward is often -1, 0, or 1.\n",
    "            avg_reward = sum(sum(ep[\"rewards\"]) for ep in completed_batch_trajectories) / len(completed_batch_trajectories) if len(completed_batch_trajectories) > 0 else 0\n",
    "            print(f\"Average reward per episode in batch: {avg_reward:.2f}\")\n",
    "\n",
    "    New_Policy.eval()   # Change to eval mode for evaluation\n",
    "\n",
    "\n",
    "    vec_env.close() # Close the environment after training\n",
    "    print(\"Training complete.\")\n",
    "    return New_Policy # Return the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5009b242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BlackJack Agent's Policy on cpu with 50 epochs, 0.0001 learning rate, batch size 64, and KL beta 0.01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 517.61it/s]\n",
      "Epoch 2/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 448.16it/s]\n",
      "Epoch 3/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 472.10it/s]\n",
      "Epoch 4/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 490.62it/s]\n",
      "Epoch 5/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 528.15it/s]\n",
      "Epoch 6/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 556.57it/s]\n",
      "Epoch 7/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 528.17it/s]\n",
      "Epoch 8/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 477.56it/s]\n",
      "Epoch 9/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 483.50it/s]\n",
      "Epoch 10/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 511.50it/s]\n",
      "Main Epoch (Outer Loop):  20%|██        | 10/50 [00:01<00:05,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: -0.0110, Ratio: 0.99524, Entropy Term: 0.35876\n",
      "Average reward per episode in batch: -0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 615.60it/s]\n",
      "Epoch 12/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 573.55it/s]\n",
      "Epoch 13/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 598.65it/s]\n",
      "Epoch 14/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 615.44it/s]\n",
      "Epoch 15/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 609.45it/s]\n",
      "Epoch 16/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 622.96it/s]\n",
      "Epoch 17/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 653.11it/s]\n",
      "Epoch 18/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 622.52it/s]\n",
      "Epoch 19/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 584.66it/s]\n",
      "Epoch 20/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 641.82it/s]\n",
      "Main Epoch (Outer Loop):  40%|████      | 20/50 [00:02<00:03,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Loss: -0.0063, Ratio: 1.00221, Entropy Term: 0.28721\n",
      "Average reward per episode in batch: -0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 653.10it/s]\n",
      "Epoch 22/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 608.77it/s]\n",
      "Epoch 23/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 627.92it/s]\n",
      "Epoch 24/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 620.55it/s]\n",
      "Epoch 25/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 614.03it/s]\n",
      "Epoch 26/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 584.60it/s]\n",
      "Epoch 27/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 600.39it/s]\n",
      "Epoch 28/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 600.34it/s]\n",
      "Epoch 29/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 621.77it/s]\n",
      "Epoch 30/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 577.23it/s]\n",
      "Main Epoch (Outer Loop):  60%|██████    | 30/50 [00:03<00:02,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Loss: -0.0112, Ratio: 0.99839, Entropy Term: 0.29060\n",
      "Average reward per episode in batch: -0.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 609.67it/s]\n",
      "Epoch 32/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 596.59it/s]\n",
      "Epoch 33/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 616.18it/s]\n",
      "Epoch 34/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 622.35it/s]\n",
      "Epoch 35/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 606.46it/s]\n",
      "Epoch 36/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 586.03it/s]\n",
      "Epoch 37/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 615.86it/s]\n",
      "Epoch 38/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 647.81it/s]\n",
      "Epoch 39/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 614.55it/s]\n",
      "Epoch 40/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 605.74it/s]\n",
      "Main Epoch (Outer Loop):  80%|████████  | 40/50 [00:04<00:01,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Loss: -0.0134, Ratio: 0.99445, Entropy Term: 0.26544\n",
      "Average reward per episode in batch: -0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 629.17it/s]\n",
      "Epoch 42/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 557.60it/s]\n",
      "Epoch 43/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 661.70it/s]\n",
      "Epoch 44/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 628.00it/s]\n",
      "Epoch 45/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 654.32it/s]\n",
      "Epoch 46/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 605.13it/s]\n",
      "Epoch 47/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 602.86it/s]\n",
      "Epoch 48/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 628.72it/s]\n",
      "Epoch 49/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 599.55it/s]\n",
      "Epoch 50/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 578.43it/s]\n",
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Loss: -0.0075, Ratio: 1.00002, Entropy Term: 0.29992\n",
      "Average reward per episode in batch: 0.07\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "_ = training_blackjack_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e3c9b5",
   "metadata": {},
   "source": [
    "Training BlackJack Agent's Policy on cpu with 50 epochs, 0.0001 learning rate, batch size 64, and KL beta 0.01.\n",
    "                                                               \n",
    "(array([21, 12, 19, 19, 20, 15, 18, 16, 10,  7, 19, 10, 17, 15, 14,  9]), array([ 5,  5,  1,  4, 10, 10,  3,  9,  2,  6,  1,  6,  9,  5,  9,  8]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d00d7a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BlackJack Agent's Policy on cuda with 50 epochs, 0.0001 learning rate, batch size 64, and KL beta 0.01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 155.30it/s]\n",
      "Epoch 2/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 256.06it/s]\n",
      "Epoch 3/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 278.24it/s]\n",
      "Epoch 4/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 284.68it/s]\n",
      "Epoch 5/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 307.77it/s]\n",
      "Epoch 6/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 318.34it/s]\n",
      "Epoch 7/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 342.41it/s]\n",
      "Epoch 8/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 304.59it/s]\n",
      "Epoch 9/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 324.50it/s]\n",
      "Epoch 10/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 304.27it/s]\n",
      "Main Epoch (Outer Loop):  20%|██        | 10/50 [00:02<00:09,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: -0.0106, Ratio: 0.99860, Entropy Term: 0.42374\n",
      "Average reward per episode in batch: -0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 322.07it/s]\n",
      "Epoch 12/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 326.85it/s]\n",
      "Epoch 13/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 303.20it/s]\n",
      "Epoch 14/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 313.83it/s]\n",
      "Epoch 15/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 320.23it/s]\n",
      "Epoch 16/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 304.54it/s]\n",
      "Epoch 17/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 334.44it/s]\n",
      "Epoch 18/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 307.94it/s]\n",
      "Epoch 19/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 330.05it/s]\n",
      "Epoch 20/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 305.78it/s]\n",
      "Main Epoch (Outer Loop):  40%|████      | 20/50 [00:04<00:06,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Loss: -0.0095, Ratio: 0.99948, Entropy Term: 0.29227\n",
      "Average reward per episode in batch: -0.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 323.01it/s]\n",
      "Epoch 22/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 312.17it/s]\n",
      "Epoch 23/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 322.06it/s]\n",
      "Epoch 24/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 297.45it/s]\n",
      "Epoch 25/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 300.94it/s]\n",
      "Epoch 26/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 317.61it/s]\n",
      "Epoch 27/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 320.67it/s]\n",
      "Epoch 28/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 336.81it/s]\n",
      "Epoch 29/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 329.01it/s]\n",
      "Epoch 30/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 321.81it/s]\n",
      "Main Epoch (Outer Loop):  60%|██████    | 30/50 [00:07<00:04,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Loss: -0.0030, Ratio: 0.99875, Entropy Term: 0.16488\n",
      "Average reward per episode in batch: -0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 318.62it/s]\n",
      "Epoch 32/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 338.63it/s]\n",
      "Epoch 33/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 311.90it/s]\n",
      "Epoch 34/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 288.53it/s]\n",
      "Epoch 35/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 334.05it/s]\n",
      "Epoch 36/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 317.15it/s]\n",
      "Epoch 37/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 327.81it/s]\n",
      "Epoch 38/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 294.92it/s]\n",
      "Epoch 39/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 316.69it/s]\n",
      "Epoch 40/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 272.92it/s]\n",
      "Main Epoch (Outer Loop):  80%|████████  | 40/50 [00:09<00:02,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Loss: -0.0162, Ratio: 0.98002, Entropy Term: 0.21612\n",
      "Average reward per episode in batch: -0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 302.20it/s]\n",
      "Epoch 42/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 299.70it/s]\n",
      "Epoch 43/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 297.24it/s]\n",
      "Epoch 44/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 307.48it/s]\n",
      "Epoch 45/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 296.50it/s]\n",
      "Epoch 46/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 329.87it/s]\n",
      "Epoch 47/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 291.51it/s]\n",
      "Epoch 48/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 309.53it/s]\n",
      "Epoch 49/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 308.86it/s]\n",
      "Epoch 50/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 313.46it/s]\n",
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Loss: -0.0038, Ratio: 0.99892, Entropy Term: 0.26437\n",
      "Average reward per episode in batch: -0.04\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "_ = training_blackjack_agent(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714c80d",
   "metadata": {},
   "source": [
    "Training BlackJack Agent's Policy with 10 epochs, 0.0001 learning rate, batch size 4, and KL beta 0.01.\n",
    "* Batch of Trajectories:\n",
    "* [{'states': [(12, 10, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1239])]}, \n",
    "* {'states': [(20, 7, 0)], 'actions': [0], 'rewards': [1.0], 'log_probs': [tensor([-0.0815])]}, \n",
    "* {'states': [(12, 1, 0), (17, 1, 0)], 'actions': [1, 1], 'rewards': [0.0, -1.0], 'log_probs': [tensor([-1.5968]), tensor([-1.9474])]}, \n",
    "* {'states': [(6, 6, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.2144])]}, \n",
    "* {'states': [(7, 4, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.2734])]}, \n",
    "* {'states': [(13, 3, 1)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1471])]}, \n",
    "* {'states': [(15, 10, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1000])]}, \n",
    "* {'states': [(12, 10, 0)], 'actions': [0], 'rewards': [1.0], 'log_probs': [tensor([-0.1239])]}, \n",
    "* {'states': [(14, 7, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1320])]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BlackJack Agent's Policy on cuda with 2000 epochs, 0.0003 learning rate, batch size 2048, and KL beta 0.01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):   5%|▌         | 100/2000 [05:15<1:45:36,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/2000, Loss: -0.0027, Ratio: 0.99872, Entropy Term: 0.13661\n",
      "Average reward per episode in batch: -0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# You can adjust these parameters as needed\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Using a larger batch_size for more stable training and to reduce empty batch issues\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)   \u001b[38;5;66;03m# Device Agnostic Code\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     trained_policy \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_blackjack_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0003\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Significantly larger batch size recommended for stability\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta_kl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mentropy_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTesting the trained policy:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m     test_env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlackjack-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, sab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[13], line 118\u001b[0m, in \u001b[0;36mtraining_blackjack_agent\u001b[1;34m(epochs, learning_rate, batch_size, gamma, k_epochs, epsilon, beta_kl, max_grad_norm, entropy_coeff, log_iterations, device)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k_epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(k_epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Inner K-Epochs)\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    117\u001b[0m     new_logits \u001b[38;5;241m=\u001b[39m New_Policy(all_states_tensor)\n\u001b[1;32m--> 118\u001b[0m     new_dist \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     new_log_probs \u001b[38;5;241m=\u001b[39m new_dist\u001b[38;5;241m.\u001b[39mlog_prob(all_actions_tensor)\n\u001b[0;32m    120\u001b[0m     entropy \u001b[38;5;241m=\u001b[39m new_dist\u001b[38;5;241m.\u001b[39mentropy()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;66;03m# Calculate entropy for regularization\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\distributions\\categorical.py:72\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     69\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     71\u001b[0m )\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\distributions\\distribution.py:69\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[0;32m     68\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[1;32m---> 69\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\distributions\\constraints.py:244\u001b[0m, in \u001b[0;36m_IndependentConstraint.check\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m--> 244\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_constraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinterpreted_batch_ndims:\n\u001b[0;32m    246\u001b[0m         expected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_constraint\u001b[38;5;241m.\u001b[39mevent_dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinterpreted_batch_ndims\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage (assuming you have a way to call this function, e.g., in a main block)\n",
    "if __name__ == '__main__':\n",
    "    # You can adjust these parameters as needed\n",
    "    # Using a larger batch_size for more stable training and to reduce empty batch issues\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   # Device Agnostic Code\n",
    "    trained_policy = training_blackjack_agent(\n",
    "        epochs=2000,\n",
    "        learning_rate=0.0003,\n",
    "        batch_size=2048, # Significantly larger batch size recommended for stability\n",
    "        k_epochs=128,\n",
    "        epsilon=0.2,\n",
    "        beta_kl=0.01,\n",
    "        entropy_coeff=0.001,\n",
    "        log_iterations=100,\n",
    "        gamma=0.99,\n",
    "        device=device,\n",
    "        num_envs=16\n",
    "    )\n",
    "\n",
    "    print(\"\\nTesting the trained policy:\")\n",
    "    test_env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "    total_test_rewards = 0\n",
    "    num_test_episodes = 1000\n",
    "\n",
    "    for _ in range(num_test_episodes):\n",
    "        obs, _ = test_env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        while not done and not truncated:\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = trained_policy(obs_tensor)\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                action = dist.sample()\n",
    "            obs, reward, done, truncated, _ = test_env.step(action.item())\n",
    "            episode_reward += reward\n",
    "        total_test_rewards += episode_reward\n",
    "\n",
    "    print(f\"Average reward over {num_test_episodes} test episodes: {total_test_rewards / num_test_episodes:.4f}\")\n",
    "    test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa240d62",
   "metadata": {},
   "source": [
    "took 32 minutes to run using the CPU\n",
    "\n",
    "Parameters: \n",
    "\n",
    "\n",
    "epochs=2000,\n",
    "        learning_rate=0.0003,\n",
    "        batch_size=2048, # Significantly larger batch size recommended for stability\n",
    "        k_epochs=128,\n",
    "        epsilon=0.2,\n",
    "        beta_kl=0.01,\n",
    "        entropy_coeff=0.001,\n",
    "        log_iterations=100,\n",
    "        gamma=0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bbb0cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5134104",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlackjack-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m\"\u001b[39m, sab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m total_test_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"Blackjack-v1\", render_mode=\"rgb\", sab=True)\n",
    "total_test_rewards = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcafcfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_episodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de95e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting env for episode: 0\n",
      "obs_tensor: tensor([[13.,  2.,  0.]]) || Action taken: tensor([1])\n",
      "obs_tensor: tensor([[14.,  2.,  0.]]) || Action taken: tensor([1])\n",
      "Reward: -1.0 || Final Observation: (23, 2, 0)\n",
      "Resetting env for episode: 1\n",
      "obs_tensor: tensor([[10., 10.,  0.]]) || Action taken: tensor([1])\n",
      "obs_tensor: tensor([[20., 10.,  0.]]) || Action taken: tensor([0])\n",
      "Reward: 1.0 || Final Observation: (20, 10, 0)\n",
      "Resetting env for episode: 2\n",
      "obs_tensor: tensor([[18., 10.,  0.]]) || Action taken: tensor([0])\n",
      "Reward: -1.0 || Final Observation: (18, 10, 0)\n",
      "Resetting env for episode: 3\n",
      "obs_tensor: tensor([[12., 10.,  0.]]) || Action taken: tensor([1])\n",
      "Reward: -1.0 || Final Observation: (22, 10, 0)\n",
      "Resetting env for episode: 4\n",
      "obs_tensor: tensor([[21.,  9.,  1.]]) || Action taken: tensor([0])\n",
      "Reward: 1.0 || Final Observation: (21, 9, 1)\n",
      "Resetting env for episode: 5\n",
      "obs_tensor: tensor([[19.,  2.,  0.]]) || Action taken: tensor([0])\n",
      "Reward: 1.0 || Final Observation: (19, 2, 0)\n",
      "Resetting env for episode: 6\n",
      "obs_tensor: tensor([[13.,  5.,  1.]]) || Action taken: tensor([1])\n",
      "obs_tensor: tensor([[13.,  5.,  0.]]) || Action taken: tensor([1])\n",
      "obs_tensor: tensor([[18.,  5.,  0.]]) || Action taken: tensor([0])\n",
      "Reward: -1.0 || Final Observation: (18, 5, 0)\n",
      "Resetting env for episode: 7\n",
      "obs_tensor: tensor([[5., 9., 0.]]) || Action taken: tensor([1])\n",
      "obs_tensor: tensor([[15.,  9.,  0.]]) || Action taken: tensor([1])\n",
      "Reward: -1.0 || Final Observation: (24, 9, 0)\n",
      "Resetting env for episode: 8\n",
      "obs_tensor: tensor([[13., 10.,  0.]]) || Action taken: tensor([1])\n",
      "Reward: -1.0 || Final Observation: (23, 10, 0)\n",
      "Resetting env for episode: 9\n",
      "obs_tensor: tensor([[21., 10.,  1.]]) || Action taken: tensor([0])\n",
      "Reward: 1.0 || Final Observation: (21, 10, 1)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for episode in range(num_test_episodes):\n",
    "    print(f\"Resetting env for episode: {episode}\")\n",
    "    obs, _ = test_env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "    stored_obs=[]\n",
    "    while not done and not truncated:\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = trained_policy(obs_tensor)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            print(f\"obs_tensor: {obs_tensor} || Action taken: {action}\")\n",
    "        obs, reward, done, truncated, _ = test_env.step(action.item())\n",
    "        episode_reward += reward\n",
    "        if (truncated): print(\"truncated\")\n",
    "    print(f\"Reward: {episode_reward} || Final Observation: {obs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00810d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df04fdf4",
   "metadata": {},
   "source": [
    "Currently the final state which reveals what the dealer ended up with in the end is not shown. By trying to access the dealer's final hand or by adding custom logging within the environment, you'll gain the critical information needed to definitively understand the why behind each reward."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRPO_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
