{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02b061c",
   "metadata": {},
   "source": [
    "**Part 2**\n",
    "\n",
    "* Adding Device Agnostic code (GPU Training)\n",
    "* Attempting to use Softmax (Categorical Distribution) implementation instead of Sigmoid (Binary Bernoulli Distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c9daf",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "Took Way too long to run with GPU/CPU switching\n",
    "\n",
    "* High delay when transferring from CPU to GPU and vice versa.\n",
    "* Look into vectorized environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d09e02",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1301783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c997ec",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83efa5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", sab=True) # `render_mode=\"human\"` creates a pygame popup window to analyze play # `sab=True` uses the Sutton & Barto version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a083c10b",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "547b9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackJackAgent(nn.Module):\n",
    "    def __init__(self, obs_size=3, hidden_size=10, output_size=2):\n",
    "        super(BlackJackAgent, self).__init__()\n",
    "        self.layer_1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.layer_2 = nn.Linear(hidden_size, output_size)\n",
    "        self.action_probs_activation_layer = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer_1(x))\n",
    "        logits = self.layer_2(x)\n",
    "        return logits       # later use nn.Softmax to get probabilities\n",
    "\n",
    "    def get_action_probs(self, logits):\n",
    "        \"\"\"Get the probabilities of each action.\"\"\"\n",
    "        return self.action_probs_activation_layer(logits)\n",
    "    \n",
    "    def sample_action(self, action:None):\n",
    "        \"\"\"Get the probability of choosing the action\"\"\"\n",
    "        logits = self.forward(action)\n",
    "        probs = self.get_action_probs(logits)\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample().item()\n",
    "        prob_of_action = dist.log_prob(action)\n",
    "        return action, prob_of_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618481e2",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_blackjack_agent(epochs=50, learning_rate=0.0001, batch_size=64, gamma=0.99, k_epochs=64, epsilon=0.2, beta_kl=0.01, max_grad_norm=0.5, entropy_coeff=0.01, log_iterations=10, device=\"cpu\") -> BlackJackAgent: \n",
    "    print(f\"Training BlackJack Agent's Policy on {device} with {epochs} epochs, {learning_rate} learning rate, batch size {batch_size}, and KL beta {beta_kl}.\")\n",
    "\n",
    "    env = gym.make(\"Blackjack-v1\", sab=True) # # `sab=True` uses the Sutton & Barto version\n",
    "    New_Policy = BlackJackAgent().to(device)   # STEP 3 || \n",
    "    optimizer = optim.Adam(params=New_Policy.parameters(), lr=learning_rate)\n",
    "    # num_correct = 0.0\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=f\"Main Epoch (Outer Loop)\", leave=False):     # STEP 4 || \n",
    "        # STEP 3 || CREATE REFERENCE MODEL OMITTED\n",
    "        batch_trajectories = []     # Will contain a batch of trajectories\n",
    "\n",
    "        # STEP 5 || Sample a batch D_b from D --> OMITTED \n",
    "        # STEP 6 || Update the old policy model PI old <- PI new\n",
    "        Policy_Old = BlackJackAgent().to(device)\n",
    "        Policy_Old.load_state_dict(New_Policy.state_dict())\n",
    "        Policy_Old.eval()   # Prevent Gradient tracking\n",
    "\n",
    "        # --- STEP 7 Collect a Batch of Experiences ---\n",
    "        # Loop Agent prediction, recording trajectories to lists:\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            # Create local trajectory library\n",
    "            episode_trajectory = {\"states\": [], \"actions\": [], \"rewards\": [], \"log_probs\": []}\n",
    "            obs, _ = env.reset()\n",
    "            done, truncated = False, False\n",
    "            while not done and not truncated:\n",
    "                obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device) # add batch dim to feed to NN\n",
    "                with torch.no_grad():\n",
    "                    logits = Policy_Old(obs_tensor)\n",
    "                    dist = torch.distributions.Categorical(logits=logits)\n",
    "                    action = dist.sample() # Tensor of shape [1]\n",
    "                    log_prob = dist.log_prob(action)\n",
    "                    \n",
    "                next_obs, reward, done, truncated, info = env.step(action.item())\n",
    "\n",
    "                # Store episode_Trajectory\n",
    "                episode_trajectory[\"states\"].append(obs)\n",
    "                episode_trajectory[\"actions\"].append(action.item())\n",
    "                episode_trajectory[\"rewards\"].append(reward)\n",
    "                episode_trajectory[\"log_probs\"].append(log_prob)\n",
    "                \n",
    "                obs = next_obs  # Update the observation\n",
    "                if (truncated):\n",
    "                    print(\"Debug: EPISODE TRUNCATED\")\n",
    "\n",
    "            batch_trajectories.append(episode_trajectory)\n",
    "\n",
    "            # print(f\"Batch of Trajectories at current epoch:{epoch}:\\n{batch_trajectories}\")\n",
    "\n",
    "\n",
    "        # These lists will hold data from ALL episodes in the current batch for Advantage Calculation\n",
    "        all_states = []\n",
    "        all_actions = []\n",
    "        all_old_log_probs = []\n",
    "        all_discounted_rewards = []\n",
    "\n",
    "        # STEP 8 || Calculate Discounted Rewards\n",
    "        for episode_trajectory in batch_trajectories:\n",
    "            rewards = episode_trajectory[\"rewards\"]\n",
    "            states = episode_trajectory[\"states\"]\n",
    "            actions = episode_trajectory[\"actions\"]\n",
    "            log_probs = episode_trajectory[\"log_probs\"]\n",
    "            \n",
    "            discounted_reward = 0\n",
    "            returns_for_episode = []\n",
    "            for reward in reversed(rewards):\n",
    "                discounted_reward = reward + gamma * discounted_reward\n",
    "                returns_for_episode.insert(0, discounted_reward)\n",
    "\n",
    "            discounted_rewards = torch.tensor(returns_for_episode, dtype=torch.float32)\n",
    "            # print(f\"discounted_rewards size: {discounted_rewards.size()}\")\n",
    "            # Add each trajectory information for the batch\n",
    "            if states:\n",
    "                all_states.extend(states)\n",
    "                all_actions.extend(actions)\n",
    "                all_old_log_probs.extend(log_probs)\n",
    "                all_discounted_rewards.extend(discounted_rewards.tolist())\n",
    "\n",
    "        # Convert all collected batch data into PyTorch tensors\n",
    "        all_states_tensor = torch.tensor(all_states, dtype=torch.float32).to(device)\n",
    "        all_actions_tensor = torch.tensor(all_actions, dtype=torch.long).to(device)\n",
    "        # Stack individual log_prob tensors and then flatten if necessary\n",
    "        all_old_log_probs_tensor = torch.cat(all_old_log_probs).squeeze(-1).to(device) # Ensure it's a 1D tensor\n",
    "        all_discounted_rewards_tensor = torch.tensor(all_discounted_rewards, dtype=torch.float32).to(device)\n",
    "\n",
    "        # STEP 9 || Calculate the Advantage of each Time Step for each Trajectory using normalization\n",
    "        all_advantages_tensor = (all_discounted_rewards_tensor - all_discounted_rewards_tensor.mean()) / (all_discounted_rewards_tensor.std() + 1e-8)\n",
    "\n",
    "        # Detach these tensors from any computation graph history\n",
    "        # as they represent fixed data for the policy updates in k_epochs.\n",
    "        # This prevents the \"RuntimeError: Trying to backward through the graph a second time\".\n",
    "        all_states_tensor = all_states_tensor.detach()\n",
    "        all_actions_tensor = all_actions_tensor.detach()\n",
    "        all_old_log_probs_tensor = all_old_log_probs_tensor.detach()\n",
    "        all_advantages_tensor = all_advantages_tensor.detach()\n",
    "\n",
    "        New_Policy.train()  # Prepare NN for updates\n",
    "\n",
    "        # --- STEP 10 || GRPO Optimization ---\n",
    "        for k_epoch in tqdm(range(k_epochs), desc=f\"Epoch {epoch+1}/{epochs} (Inner K-Epochs)\", leave=True):\n",
    "            new_logits = New_Policy(all_states_tensor)\n",
    "            new_dist = torch.distributions.Categorical(logits=new_logits)\n",
    "            new_log_probs = new_dist.log_prob(all_actions_tensor)\n",
    "            entropy = new_dist.entropy().mean() # Calculate entropy for regularization\n",
    "\n",
    "            R1_ratio = torch.exp(new_log_probs - all_old_log_probs_tensor)\n",
    "\n",
    "            unclipped_surrogate = R1_ratio * all_advantages_tensor\n",
    "            clipped_surrogate = torch.clamp(input=R1_ratio, min=1.0-epsilon, max=1.0+epsilon) * all_advantages_tensor\n",
    "\n",
    "            policy_loss = -torch.min(unclipped_surrogate, clipped_surrogate).mean()\n",
    "\n",
    "            # --- KL Divergence Calculation ---\n",
    "            # Create distributions for old policies using the trajectory states\n",
    "            with torch.no_grad():\n",
    "                old_logits = Policy_Old(all_states_tensor)\n",
    "            old_dist = torch.distributions.Categorical(logits=old_logits)\n",
    "\n",
    "            # Calculate KL divergence per sample, then take the mean over the batch\n",
    "            kl_div_per_sample = torch.distributions.kl.kl_divergence(p=new_dist, q=old_dist)\n",
    "            kl_loss = kl_div_per_sample.mean() # Mean over the batch\n",
    "\n",
    "            # Total Loss for GRPO\n",
    "            total_loss = policy_loss + beta_kl * kl_loss - entropy_coeff * entropy\n",
    "\n",
    "            # STEP 11 || Policy Updates\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(New_Policy.parameters(), max_grad_norm)\n",
    "            optimizer.step()    # Update policy parameters using gradient ascent\n",
    "        \n",
    "        \n",
    "        # --- 4. Logging and Evaluation ---\n",
    "        if (epoch + 1) % log_iterations == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}, Ratio: {R1_ratio.mean().item():.5f}, Entropy Term: {entropy:.5f}\")\n",
    "            # You can add more evaluation metrics here, e.g., average reward per episode\n",
    "            # For Blackjack, the reward is often -1, 0, or 1.\n",
    "            avg_reward = sum(sum(ep[\"rewards\"]) for ep in batch_trajectories) / batch_size\n",
    "            print(f\"Average reward per episode in batch: {avg_reward:.2f}\")\n",
    "\n",
    "    New_Policy.eval()   # Change to eval mode for evaluation\n",
    "\n",
    "\n",
    "    env.close() # Close the environment after training\n",
    "    print(\"Training complete.\")\n",
    "    return New_Policy # Return the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5009b242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BlackJack Agent's Policy on cpu with 50 epochs, 0.0001 learning rate, batch size 64, and KL beta 0.01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 607.30it/s]\n",
      "Epoch 2/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 602.34it/s]\n",
      "Epoch 3/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 670.68it/s]\n",
      "Epoch 4/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 652.80it/s]\n",
      "Epoch 5/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 672.69it/s]\n",
      "Epoch 6/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 678.68it/s]\n",
      "Epoch 7/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 679.65it/s]\n",
      "Epoch 8/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 594.22it/s]\n",
      "Epoch 9/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 732.53it/s]\n",
      "Epoch 10/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 633.22it/s]\n",
      "Main Epoch (Outer Loop):  20%|██        | 10/50 [00:01<00:04,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: -0.0342, Ratio: 1.01189, Entropy Term: 0.43756\n",
      "Average reward per episode in batch: -0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 616.12it/s]\n",
      "Epoch 12/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 674.66it/s]\n",
      "Epoch 13/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 651.96it/s]\n",
      "Epoch 14/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 712.00it/s]\n",
      "Epoch 15/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 696.99it/s]\n",
      "Epoch 16/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 703.84it/s]\n",
      "Epoch 17/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 668.70it/s]\n",
      "Epoch 18/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 657.33it/s]\n",
      "Epoch 19/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 726.12it/s]\n",
      "Epoch 20/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 672.24it/s]\n",
      "Main Epoch (Outer Loop):  40%|████      | 20/50 [00:02<00:03,  9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Loss: -0.0081, Ratio: 0.99732, Entropy Term: 0.34493\n",
      "Average reward per episode in batch: -0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 621.21it/s]\n",
      "Epoch 22/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 609.00it/s]\n",
      "Epoch 23/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 629.88it/s]\n",
      "Epoch 24/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 604.21it/s]\n",
      "Epoch 25/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 693.77it/s]\n",
      "Epoch 26/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 747.40it/s]\n",
      "Epoch 27/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 594.10it/s]\n",
      "Epoch 28/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 617.33it/s]\n",
      "Epoch 29/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 654.89it/s]\n",
      "Epoch 30/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 673.82it/s]\n",
      "Main Epoch (Outer Loop):  60%|██████    | 30/50 [00:03<00:02,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Loss: -0.0047, Ratio: 0.99824, Entropy Term: 0.25626\n",
      "Average reward per episode in batch: -0.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 645.77it/s]\n",
      "Epoch 32/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 764.95it/s]\n",
      "Epoch 33/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 654.60it/s]\n",
      "Epoch 34/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 613.97it/s]\n",
      "Epoch 35/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 654.14it/s]\n",
      "Epoch 36/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 617.80it/s]\n",
      "Epoch 37/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 618.29it/s]\n",
      "Epoch 38/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 625.57it/s]\n",
      "Epoch 39/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 652.24it/s]\n",
      "Epoch 40/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 659.58it/s]\n",
      "Main Epoch (Outer Loop):  80%|████████  | 40/50 [00:04<00:01,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Loss: -0.0114, Ratio: 0.99346, Entropy Term: 0.10141\n",
      "Average reward per episode in batch: 0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 582.63it/s]\n",
      "Epoch 42/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 636.18it/s]\n",
      "Epoch 43/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 590.60it/s]\n",
      "Epoch 44/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 628.22it/s]\n",
      "Epoch 45/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 595.57it/s]\n",
      "Epoch 46/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 657.62it/s]\n",
      "Epoch 47/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 638.46it/s]\n",
      "Epoch 48/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 607.83it/s]\n",
      "Epoch 49/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 627.26it/s]\n",
      "Epoch 50/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 645.98it/s]\n",
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Loss: -0.0028, Ratio: 0.99890, Entropy Term: 0.05628\n",
      "Average reward per episode in batch: -0.22\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "_ = training_blackjack_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d00d7a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BlackJack Agent's Policy on cuda with 50 epochs, 0.0001 learning rate, batch size 64, and KL beta 0.01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 299.76it/s]\n",
      "Epoch 2/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 352.32it/s]\n",
      "Epoch 3/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 330.28it/s]\n",
      "Epoch 4/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 353.04it/s]\n",
      "Epoch 5/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 319.99it/s]\n",
      "Epoch 6/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 324.98it/s]\n",
      "Epoch 7/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 313.28it/s]\n",
      "Epoch 8/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 299.54it/s]\n",
      "Epoch 9/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 314.98it/s]\n",
      "Epoch 10/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 311.24it/s]\n",
      "Main Epoch (Outer Loop):  20%|██        | 10/50 [00:03<00:12,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: -0.0004, Ratio: 1.00033, Entropy Term: 0.01681\n",
      "Average reward per episode in batch: -1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 307.76it/s]\n",
      "Epoch 12/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 319.51it/s]\n",
      "Epoch 13/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 324.89it/s]\n",
      "Epoch 14/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 306.38it/s]\n",
      "Epoch 15/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 319.08it/s]\n",
      "Epoch 16/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 303.61it/s]\n",
      "Epoch 17/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 318.14it/s]\n",
      "Epoch 18/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 308.87it/s]\n",
      "Epoch 19/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 288.70it/s]\n",
      "Epoch 20/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 281.01it/s]\n",
      "Main Epoch (Outer Loop):  40%|████      | 20/50 [00:06<00:09,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Loss: -0.0001, Ratio: 1.00005, Entropy Term: 0.00639\n",
      "Average reward per episode in batch: -1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 287.67it/s]\n",
      "Epoch 22/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 307.63it/s]\n",
      "Epoch 23/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 281.05it/s]\n",
      "Epoch 24/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 286.78it/s]\n",
      "Epoch 25/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 315.25it/s]\n",
      "Epoch 26/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 311.94it/s]\n",
      "Epoch 27/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 332.44it/s]\n",
      "Epoch 28/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 313.90it/s]\n",
      "Epoch 29/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 302.64it/s]\n",
      "Epoch 30/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 307.65it/s]\n",
      "Main Epoch (Outer Loop):  60%|██████    | 30/50 [00:09<00:06,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Loss: -0.0001, Ratio: 1.00003, Entropy Term: 0.00348\n",
      "Average reward per episode in batch: -1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 337.54it/s]\n",
      "Epoch 32/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 292.97it/s]\n",
      "Epoch 33/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 298.01it/s]\n",
      "Epoch 34/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 291.45it/s]\n",
      "Epoch 35/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 299.27it/s]\n",
      "Epoch 36/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 282.37it/s]\n",
      "Epoch 37/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 330.69it/s]\n",
      "Epoch 38/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 306.17it/s]\n",
      "Epoch 39/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 285.39it/s]\n",
      "Epoch 40/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 324.32it/s]\n",
      "Main Epoch (Outer Loop):  80%|████████  | 40/50 [00:13<00:03,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Loss: -0.0001, Ratio: 1.00003, Entropy Term: 0.00336\n",
      "Average reward per episode in batch: -1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 319.75it/s]\n",
      "Epoch 42/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 337.10it/s]\n",
      "Epoch 43/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 318.36it/s]\n",
      "Epoch 44/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 338.02it/s]\n",
      "Epoch 45/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 325.59it/s]\n",
      "Epoch 46/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 326.61it/s]\n",
      "Epoch 47/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 303.96it/s]\n",
      "Epoch 48/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 347.19it/s]\n",
      "Epoch 49/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 280.39it/s]\n",
      "Epoch 50/50 (Inner K-Epochs): 100%|██████████| 64/64 [00:00<00:00, 314.26it/s]\n",
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Loss: -0.0000, Ratio: 1.00003, Entropy Term: 0.00177\n",
      "Average reward per episode in batch: -1.00\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "_ = training_blackjack_agent(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714c80d",
   "metadata": {},
   "source": [
    "Training BlackJack Agent's Policy with 10 epochs, 0.0001 learning rate, batch size 4, and KL beta 0.01.\n",
    "* Batch of Trajectories:\n",
    "* [{'states': [(12, 10, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1239])]}, \n",
    "* {'states': [(20, 7, 0)], 'actions': [0], 'rewards': [1.0], 'log_probs': [tensor([-0.0815])]}, \n",
    "* {'states': [(12, 1, 0), (17, 1, 0)], 'actions': [1, 1], 'rewards': [0.0, -1.0], 'log_probs': [tensor([-1.5968]), tensor([-1.9474])]}, \n",
    "* {'states': [(6, 6, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.2144])]}, \n",
    "* {'states': [(7, 4, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.2734])]}, \n",
    "* {'states': [(13, 3, 1)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1471])]}, \n",
    "* {'states': [(15, 10, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1000])]}, \n",
    "* {'states': [(12, 10, 0)], 'actions': [0], 'rewards': [1.0], 'log_probs': [tensor([-0.1239])]}, \n",
    "* {'states': [(14, 7, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1320])]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc91f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BlackJack Agent's Policy on cuda with 2000 epochs, 0.0003 learning rate, batch size 2048, and KL beta 0.01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):   5%|▌         | 100/2000 [05:15<1:45:36,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/2000, Loss: -0.0027, Ratio: 0.99872, Entropy Term: 0.13661\n",
      "Average reward per episode in batch: -0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# You can adjust these parameters as needed\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Using a larger batch_size for more stable training and to reduce empty batch issues\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)   \u001b[38;5;66;03m# Device Agnostic Code\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     trained_policy \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_blackjack_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0003\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Significantly larger batch size recommended for stability\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta_kl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mentropy_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTesting the trained policy:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m     test_env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlackjack-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, sab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[13], line 118\u001b[0m, in \u001b[0;36mtraining_blackjack_agent\u001b[1;34m(epochs, learning_rate, batch_size, gamma, k_epochs, epsilon, beta_kl, max_grad_norm, entropy_coeff, log_iterations, device)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k_epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(k_epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Inner K-Epochs)\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    117\u001b[0m     new_logits \u001b[38;5;241m=\u001b[39m New_Policy(all_states_tensor)\n\u001b[1;32m--> 118\u001b[0m     new_dist \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     new_log_probs \u001b[38;5;241m=\u001b[39m new_dist\u001b[38;5;241m.\u001b[39mlog_prob(all_actions_tensor)\n\u001b[0;32m    120\u001b[0m     entropy \u001b[38;5;241m=\u001b[39m new_dist\u001b[38;5;241m.\u001b[39mentropy()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;66;03m# Calculate entropy for regularization\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\distributions\\categorical.py:72\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     69\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     71\u001b[0m )\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\distributions\\distribution.py:69\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[0;32m     68\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[1;32m---> 69\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\distributions\\constraints.py:244\u001b[0m, in \u001b[0;36m_IndependentConstraint.check\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m--> 244\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_constraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinterpreted_batch_ndims:\n\u001b[0;32m    246\u001b[0m         expected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_constraint\u001b[38;5;241m.\u001b[39mevent_dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinterpreted_batch_ndims\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage (assuming you have a way to call this function, e.g., in a main block)\n",
    "if __name__ == '__main__':\n",
    "    # You can adjust these parameters as needed\n",
    "    # Using a larger batch_size for more stable training and to reduce empty batch issues\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   # Device Agnostic Code\n",
    "    trained_policy = training_blackjack_agent(\n",
    "        epochs=2000,\n",
    "        learning_rate=0.0003,\n",
    "        batch_size=2048, # Significantly larger batch size recommended for stability\n",
    "        k_epochs=128,\n",
    "        epsilon=0.2,\n",
    "        beta_kl=0.01,\n",
    "        entropy_coeff=0.001,\n",
    "        log_iterations=100,\n",
    "        gamma=0.99,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(\"\\nTesting the trained policy:\")\n",
    "    test_env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "    total_test_rewards = 0\n",
    "    num_test_episodes = 1000\n",
    "\n",
    "    for _ in range(num_test_episodes):\n",
    "        obs, _ = test_env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        while not done and not truncated:\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = trained_policy(obs_tensor)\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                action = dist.sample()\n",
    "            obs, reward, done, truncated, _ = test_env.step(action.item())\n",
    "            episode_reward += reward\n",
    "        total_test_rewards += episode_reward\n",
    "\n",
    "    print(f\"Average reward over {num_test_episodes} test episodes: {total_test_rewards / num_test_episodes:.4f}\")\n",
    "    test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa240d62",
   "metadata": {},
   "source": [
    "took 32 minutes to run using the CPU\n",
    "\n",
    "Parameters: \n",
    "\n",
    "\n",
    "epochs=2000,\n",
    "        learning_rate=0.0003,\n",
    "        batch_size=2048, # Significantly larger batch size recommended for stability\n",
    "        k_epochs=128,\n",
    "        epsilon=0.2,\n",
    "        beta_kl=0.01,\n",
    "        entropy_coeff=0.001,\n",
    "        log_iterations=100,\n",
    "        gamma=0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bbb0cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5134104",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlackjack-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m\"\u001b[39m, sab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m total_test_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"Blackjack-v1\", render_mode=\"rgb\", sab=True)\n",
    "total_test_rewards = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcafcfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_episodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de95e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting env for episode: 0\n",
      "obs_tensor: tensor([[13.,  2.,  0.]]) || Action taken: tensor([1])\n",
      "obs_tensor: tensor([[14.,  2.,  0.]]) || Action taken: tensor([1])\n",
      "Reward: -1.0 || Final Observation: (23, 2, 0)\n",
      "Resetting env for episode: 1\n",
      "obs_tensor: tensor([[10., 10.,  0.]]) || Action taken: tensor([1])\n",
      "obs_tensor: tensor([[20., 10.,  0.]]) || Action taken: tensor([0])\n",
      "Reward: 1.0 || Final Observation: (20, 10, 0)\n",
      "Resetting env for episode: 2\n",
      "obs_tensor: tensor([[18., 10.,  0.]]) || Action taken: tensor([0])\n",
      "Reward: -1.0 || Final Observation: (18, 10, 0)\n",
      "Resetting env for episode: 3\n",
      "obs_tensor: tensor([[12., 10.,  0.]]) || Action taken: tensor([1])\n",
      "Reward: -1.0 || Final Observation: (22, 10, 0)\n",
      "Resetting env for episode: 4\n",
      "obs_tensor: tensor([[21.,  9.,  1.]]) || Action taken: tensor([0])\n",
      "Reward: 1.0 || Final Observation: (21, 9, 1)\n",
      "Resetting env for episode: 5\n",
      "obs_tensor: tensor([[19.,  2.,  0.]]) || Action taken: tensor([0])\n",
      "Reward: 1.0 || Final Observation: (19, 2, 0)\n",
      "Resetting env for episode: 6\n",
      "obs_tensor: tensor([[13.,  5.,  1.]]) || Action taken: tensor([1])\n",
      "obs_tensor: tensor([[13.,  5.,  0.]]) || Action taken: tensor([1])\n",
      "obs_tensor: tensor([[18.,  5.,  0.]]) || Action taken: tensor([0])\n",
      "Reward: -1.0 || Final Observation: (18, 5, 0)\n",
      "Resetting env for episode: 7\n",
      "obs_tensor: tensor([[5., 9., 0.]]) || Action taken: tensor([1])\n",
      "obs_tensor: tensor([[15.,  9.,  0.]]) || Action taken: tensor([1])\n",
      "Reward: -1.0 || Final Observation: (24, 9, 0)\n",
      "Resetting env for episode: 8\n",
      "obs_tensor: tensor([[13., 10.,  0.]]) || Action taken: tensor([1])\n",
      "Reward: -1.0 || Final Observation: (23, 10, 0)\n",
      "Resetting env for episode: 9\n",
      "obs_tensor: tensor([[21., 10.,  1.]]) || Action taken: tensor([0])\n",
      "Reward: 1.0 || Final Observation: (21, 10, 1)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for episode in range(num_test_episodes):\n",
    "    print(f\"Resetting env for episode: {episode}\")\n",
    "    obs, _ = test_env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "    stored_obs=[]\n",
    "    while not done and not truncated:\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = trained_policy(obs_tensor)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            print(f\"obs_tensor: {obs_tensor} || Action taken: {action}\")\n",
    "        obs, reward, done, truncated, _ = test_env.step(action.item())\n",
    "        episode_reward += reward\n",
    "        if (truncated): print(\"truncated\")\n",
    "    print(f\"Reward: {episode_reward} || Final Observation: {obs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00810d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df04fdf4",
   "metadata": {},
   "source": [
    "Currently the final state which reveals what the dealer ended up with in the end is not shown. By trying to access the dealer's final hand or by adding custom logging within the environment, you'll gain the critical information needed to definitively understand the why behind each reward."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRPO_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
