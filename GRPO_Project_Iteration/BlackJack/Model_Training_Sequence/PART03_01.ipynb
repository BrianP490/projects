{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02b061c",
   "metadata": {},
   "source": [
    "**Part 3**\n",
    "\n",
    "* Using this to turn into .py script file\n",
    "* Adding vectorized BlackJack Environment\n",
    "* Adding Device Agnostic code (GPU Training)\n",
    "* Attempting to use Softmax (Categorical Distribution) implementation instead of Sigmoid (Binary Bernoulli Distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c9daf",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "* Still very slow during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d09e02",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1301783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c997ec",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a083c10b",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547b9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackJackAgent(nn.Module):\n",
    "    def __init__(self, obs_size=3, hidden_size=10, output_size=2):\n",
    "        super(BlackJackAgent, self).__init__()\n",
    "        self.layer_1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.layer_2 = nn.Linear(hidden_size, output_size)\n",
    "        self.action_probs_activation_layer = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer_1(x))\n",
    "        logits = self.layer_2(x)\n",
    "        return logits       # later use nn.Softmax to get probabilities\n",
    "\n",
    "    def get_action_probs(self, logits):\n",
    "        \"\"\"Get the probabilities of each action.\"\"\"\n",
    "        return self.action_probs_activation_layer(logits)\n",
    "    \n",
    "    def sample_best_action(self, obs):\n",
    "        \"\"\"Get the deterministic action with the highest probability\n",
    "        for a given observation.\n",
    "        \n",
    "        Parameters:\n",
    "            obs (torch.tensor): the agent's current observable state in the playable environment. Expected shape is either `(num_features,)` for a single observation\n",
    "            or `(batch_size, num_features)` for a batch of observations.\n",
    "        \n",
    "        Returns:\n",
    "            action (int or torch.tensor): \n",
    "                - If `obs` is a single observation (i.e., `obs.dim() == 1`), returns a scalar `int` representing the chosen action. \n",
    "\n",
    "                - If `obs` is a batch of observations (i.e., `obs.dim() > 1`),\n",
    "                returns a `torch.Tensor` of `int`s, where each element is the\n",
    "                chosen action for the corresponding observation in the batch\"\"\"\n",
    "        # Ensure observation is a tensor and has a batch dimension if it's a single observation\n",
    "        if obs.dim() == 1:\n",
    "            obs = obs.unsqueeze(0) # Add a batch dimension if it's a single observation\n",
    "\n",
    "        logits = self.forward(obs)\n",
    "        probs = self.get_action_probs(logits)\n",
    "        action = torch.argmax(probs, dim=1) \n",
    "        if obs.size(0) == 1:    # This method checks if there is only 1 element in a 1-D tensor\n",
    "            return action.item() # Returns a Python scalar for a single observation\n",
    "        else:\n",
    "            return action # Returns a tensor of actions for a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618481e2",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_blackjack_agent(epochs=50, learning_rate=0.0001, batch_size=64, gamma=0.99, k_epochs=64, epsilon=0.2, beta_kl=0.01, max_grad_norm=0.5, entropy_coeff=0.01, log_iterations=10, device=\"cpu\", num_envs=16) -> BlackJackAgent: \n",
    "    print(f\"Training BlackJack Agent's Policy on {device} with {epochs} epochs, {learning_rate} learning rate, batch size {batch_size}, and KL beta {beta_kl}.\")\n",
    "\n",
    "    vec_env = gym.make_vec(\"Blackjack-v1\", num_envs=num_envs, sab=True) # `sab=True` uses the Sutton & Barto version\n",
    "\n",
    "    New_Policy = BlackJackAgent().to(device)   # STEP 3 || \n",
    "    optimizer = optim.Adam(params=New_Policy.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=f\"Main Epoch (Outer Loop)\", leave=False):     # STEP 4 || \n",
    "        # STEP 5 || Sample a batch D_b from D --> OMITTED \n",
    "        # STEP 6 || Update the old policy model PI old <- PI new\n",
    "        Policy_Old = BlackJackAgent().to(device)\n",
    "        Policy_Old.load_state_dict(New_Policy.state_dict())\n",
    "        Policy_Old.eval()   # Prevent Gradient tracking\n",
    "\n",
    "        # This will store trajectories for all episodes collected in the current batch\n",
    "        completed_batch_trajectories = []\n",
    "\n",
    "        # Reset all vectorized environments\n",
    "        raw_observations, infos = vec_env.reset() # observations is a numpy array of shape (num_envs, obs_dim(3))\n",
    "        observations = np.stack(raw_observations, axis=1)\n",
    "        dones = np.array([False] * num_envs) # Track the done status for each parallel environment\n",
    "        truncateds = np.array([False] * num_envs) # Track truncated status for each parallel environment\n",
    "\n",
    "        # Initialize current trajectories for all parallel environments\n",
    "        # Each element in this list will be a dict for an *in-progress* episode in a specific env\n",
    "        current_episode_trajectories = [{\"states\": [], \"actions\": [], \"rewards\": [], \"log_probs\": []} for _ in range(num_envs)]\n",
    "\n",
    "        # --- STEP 7 Collect a Batch of Experiences Using the Old Policy---\n",
    "        # Loop Agent prediction, recording trajectories to lists:\n",
    "        episodes_collected_in_batch = 0\n",
    "        max_steps_per_batch_limit = batch_size * 5 # A safety limit to prevent infinite loops if episodes are very long\n",
    "        current_total_steps = 0\n",
    "\n",
    "        while episodes_collected_in_batch < batch_size and current_total_steps < max_steps_per_batch_limit:\n",
    "            obs_tensor = torch.tensor(observations, dtype=torch.float32).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = Policy_Old(obs_tensor)\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                actions = dist.sample() # Tensor of shape [1]\n",
    "                log_probs = dist.log_prob(actions)\n",
    "                    \n",
    "            raw_next_obs, rewards, dones, truncateds, infos = vec_env.step(actions.cpu().numpy()) # actions must be on CPU for env.step()\n",
    "            next_obs = np.stack(raw_next_obs, axis=1)\n",
    "            current_total_steps += num_envs\n",
    "\n",
    "            # Process data for each parallel environment\n",
    "            for env_idx in range(num_envs):\n",
    "                \n",
    "                obs_to_append = observations[env_idx]\n",
    "                if isinstance(obs_to_append, torch.Tensor):\n",
    "                    obs_to_append = obs_to_append.cpu().numpy()\n",
    "                # Store current_episode_trajectories\n",
    "                current_episode_trajectories[env_idx][\"states\"].append(obs_to_append)\n",
    "                current_episode_trajectories[env_idx][\"actions\"].append(actions[env_idx].item())\n",
    "                current_episode_trajectories[env_idx][\"rewards\"].append(rewards[env_idx])\n",
    "                current_episode_trajectories[env_idx][\"log_probs\"].append(log_probs[env_idx].cpu())\n",
    "                \n",
    "                if dones[env_idx] or truncateds[env_idx]:\n",
    "                    completed_batch_trajectories.append(current_episode_trajectories[env_idx])\n",
    "                    episodes_collected_in_batch += 1\n",
    "\n",
    "                    current_episode_trajectories[env_idx] = {\"states\": [], \"actions\": [], \"rewards\": [], \"log_probs\": []}\n",
    "\n",
    "            observations = next_obs  # Update the observation\n",
    "\n",
    "        for env_idx in range(num_envs):\n",
    "            if len(current_episode_trajectories[env_idx][\"states\"]) > 0:\n",
    "                # For simplicity for now, we'll append the incomplete training sequences. In full PPO, you'd add\n",
    "                # the value of the last state to its rewards.\n",
    "                completed_batch_trajectories.append(current_episode_trajectories[env_idx])\n",
    "                # Note: These might not be \"full\" episodes in the sense of reaching a done state,\n",
    "                # but they contribute steps to your batch.\n",
    "\n",
    "        # These lists will hold data from ALL episodes in the current batch for Advantage Calculation\n",
    "        all_states = []\n",
    "        all_actions = []\n",
    "        all_old_log_probs = []\n",
    "        all_discounted_rewards = []\n",
    "\n",
    "        # STEP 8 || Calculate Discounted Rewards for completed trajectories\n",
    "        for episode_trajectory in completed_batch_trajectories: \n",
    "            rewards = episode_trajectory[\"rewards\"]\n",
    "            states = episode_trajectory[\"states\"]\n",
    "            actions = episode_trajectory[\"actions\"]\n",
    "            log_probs = episode_trajectory[\"log_probs\"]\n",
    "            \n",
    "            if not rewards:\n",
    "                continue\n",
    "\n",
    "            discounted_reward = 0\n",
    "            returns_for_episode = []\n",
    "            for reward in reversed(rewards):\n",
    "                discounted_reward = reward + gamma * discounted_reward\n",
    "                returns_for_episode.insert(0, discounted_reward)\n",
    "\n",
    "            discounted_rewards = torch.tensor(returns_for_episode, dtype=torch.float32)\n",
    "            # Add each trajectory information for the batch\n",
    "            if states:\n",
    "                all_states.extend(states)\n",
    "                all_actions.extend(actions)\n",
    "                all_old_log_probs.extend(log_probs)\n",
    "                all_discounted_rewards.extend(discounted_rewards.tolist())\n",
    "\n",
    "        # --- IMPORTANT: Pre-tensorization checks and conversions ---\n",
    "        if not all_states or not all_actions or not all_old_log_probs or not all_discounted_rewards:\n",
    "            print(f\"Warning: Epoch {epoch + 1}: Insufficient data collected for optimization. \"\n",
    "                  f\"Skipping policy update for this epoch.\")\n",
    "            print(f\"  Counts: States={len(all_states)}, Actions={len(all_actions)}, \"\n",
    "                  f\"LogProbs={len(all_old_log_probs)}, Rewards={len(all_discounted_rewards)}\")\n",
    "            continue\n",
    "        # Convert all collected batch data into PyTorch tensors\n",
    "        all_states_tensor = torch.tensor(np.array(all_states), dtype=torch.float32).to(device)\n",
    "        all_actions_tensor = torch.tensor(all_actions, dtype=torch.long).to(device)\n",
    "        # Stack individual log_prob tensors and then flatten if necessary\n",
    "        all_old_log_probs_tensor = torch.tensor(all_old_log_probs, dtype=torch.float32).to(device) # Ensure it's a 1D tensor\n",
    "        all_discounted_rewards_tensor = torch.tensor(all_discounted_rewards, dtype=torch.float32).to(device)\n",
    "\n",
    "        # STEP 9 || Calculate the Advantage of each Time Step for each Trajectory using normalization\n",
    "        all_advantages_tensor = (all_discounted_rewards_tensor - all_discounted_rewards_tensor.mean()) / (all_discounted_rewards_tensor.std() + 1e-8)\n",
    "\n",
    "        # Detach these tensors from any computation graph history\n",
    "        # as they represent fixed data for the policy updates in k_epochs.\n",
    "        # This prevents the \"RuntimeError: Trying to backward through the graph a second time\".\n",
    "        all_states_tensor = all_states_tensor.detach()\n",
    "        all_actions_tensor = all_actions_tensor.detach()\n",
    "        all_old_log_probs_tensor = all_old_log_probs_tensor.detach()\n",
    "        all_advantages_tensor = all_advantages_tensor.detach()\n",
    "\n",
    "        New_Policy.train()  # Prepare NN for updates\n",
    "\n",
    "        # --- STEP 10 || GRPO Optimization ---\n",
    "        for k_epoch in tqdm(range(k_epochs), desc=f\"Epoch {epoch+1}/{epochs} (Inner K-Epochs)\", leave=True):\n",
    "            new_logits = New_Policy(all_states_tensor)\n",
    "            new_dist = torch.distributions.Categorical(logits=new_logits)\n",
    "            new_log_probs = new_dist.log_prob(all_actions_tensor)\n",
    "            entropy = new_dist.entropy().mean() # Calculate entropy for regularization\n",
    "\n",
    "            R1_ratio = torch.exp(new_log_probs - all_old_log_probs_tensor)\n",
    "\n",
    "            unclipped_surrogate = R1_ratio * all_advantages_tensor\n",
    "            clipped_surrogate = torch.clamp(input=R1_ratio, min=1.0-epsilon, max=1.0+epsilon) * all_advantages_tensor\n",
    "\n",
    "            policy_loss = -torch.min(unclipped_surrogate, clipped_surrogate).mean()\n",
    "\n",
    "            # --- KL Divergence Calculation ---\n",
    "            # Create distributions for old policies using the trajectory states\n",
    "            with torch.no_grad():\n",
    "                old_logits = Policy_Old(all_states_tensor)\n",
    "            old_dist = torch.distributions.Categorical(logits=old_logits)\n",
    "\n",
    "            # Calculate KL divergence per sample, then take the mean over the batch\n",
    "            kl_div_per_sample = torch.distributions.kl.kl_divergence(p=new_dist, q=old_dist)\n",
    "            kl_loss = kl_div_per_sample.mean() # Mean over the batch\n",
    "\n",
    "            # Total Loss for GRPO\n",
    "            total_loss = policy_loss + beta_kl * kl_loss - entropy_coeff * entropy\n",
    "\n",
    "            # STEP 11 || Policy Updates\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(New_Policy.parameters(), max_grad_norm)\n",
    "            optimizer.step()    # Update policy parameters using gradient ascent\n",
    "        \n",
    "        \n",
    "        # --- 4. Logging and Evaluation ---\n",
    "        if (epoch + 1) % log_iterations == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}, Ratio: {R1_ratio.mean().item():.5f}, Entropy Term: {entropy:.5f}\")\n",
    "            # You can add more evaluation metrics here, e.g., average reward per episode\n",
    "            # For Blackjack, the reward is often -1, 0, or 1.\n",
    "            avg_reward = sum(sum(ep[\"rewards\"]) for ep in completed_batch_trajectories) / len(completed_batch_trajectories) if len(completed_batch_trajectories) > 0 else 0\n",
    "            print(f\"Average reward per episode in batch: {avg_reward:.2f}\")\n",
    "\n",
    "    New_Policy.eval()   # Change to eval mode for evaluation\n",
    "\n",
    "\n",
    "    vec_env.close() # Close the environment after training\n",
    "    print(\"Training complete.\")\n",
    "    return New_Policy # Return the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5009b242",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = training_blackjack_agent(epochs=50, learning_rate=0.0001, batch_size=64, gamma=0.99, k_epochs=64, epsilon=0.2, beta_kl=0.01, max_grad_norm=0.5, entropy_coeff=0.01, log_iterations=10, device=\"cpu\", num_envs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb393851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print(\"Beginning Training Script\")\n",
    "    \n",
    "    if args.device:     # Check if the user specified to use a CPU or GPU for training\n",
    "        device = args.device\n",
    "    else:\n",
    "        if args.use_cuda:   # Check if the user wanted to use CUDA if available.\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    start_time=time.time()\n",
    "    trained_policy = training_blackjack_agent(\n",
    "        epochs=args.epochs,\n",
    "        learning_rate=args.learning_rate,\n",
    "        batch_size=args.batch_size, # Significantly larger batch size recommended for stability\n",
    "        k_epochs=args.k_epochs,\n",
    "        epsilon=args.epsilon,\n",
    "        beta_kl=args.beta_kl,\n",
    "        entropy_coeff=args.entropy_coeff,\n",
    "        log_iterations=args.log_iterations,\n",
    "        gamma=args.gamma,\n",
    "        device=device,\n",
    "        num_envs=args.num_envs\n",
    "    )\n",
    "    end_time=time.time()\n",
    "\n",
    "    elapsed_time= end_time - start_time\n",
    "    hrs = int(elapsed_time / 3600)\n",
    "    min = int((elapsed_time % 3600) / 60)\n",
    "    seconds_remaining = elapsed_time - (hrs * 3600 ) - (min * 60)\n",
    "    print(f\"FINISHED MODEL TRAINING. \\nTRAINING TOOK: {hrs} Hours, {min} Minutes, and {seconds_remaining} Seconds\")\n",
    "\n",
    "\n",
    "    print(\"\\nTesting the trained policy:\")\n",
    "    test_env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "    total_test_rewards = 0\n",
    "    num_test_episodes = 1000\n",
    "\n",
    "    for _ in range(num_test_episodes):\n",
    "        obs, _ = test_env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        while not done and not truncated:\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                action = trained_policy.sample_best_action(obs_tensor)\n",
    "                print(f\"obs_tensor: {obs_tensor} || Action taken: {action}\")\n",
    "            obs, reward, done, truncated, _ = test_env.step(action)\n",
    "            episode_reward += reward\n",
    "        total_test_rewards += episode_reward\n",
    "        print(f\"Reward: {episode_reward} || Final Observation before reward: {obs}\")\n",
    "\n",
    "    print(f\"\\nAverage reward over {num_test_episodes} test episodes: {total_test_rewards / num_test_episodes:.4f}\")\n",
    "    test_env.close()    # safely close the gym environment after the testing and validation of the trained model\n",
    "\n",
    "    #---------------  !!!  ---------------\n",
    "    SAVE_LOCATION = \"../app/model_weights/blackjack_policy_model.pth\"   # Define the model path and name of the trained model weights\n",
    "\n",
    "    if args.save_model:     # Check if the user wants to save the trained model weights\n",
    "        if args.model_output_path:     # Check if the user specified a target save location\n",
    "            SAVE_LOCATION=args.model_output_path\n",
    "        \n",
    "        torch.save(trained_policy.parameters(), f=SAVE_LOCATION)\n",
    "        print(f\"Model weights saved in: {SAVE_LOCATION}\")\n",
    "\n",
    "    print(\"Finished Running Script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (assuming you have a way to call this function, e.g., in a main block)\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"Train and test a BlackJack PPO agent.\")\n",
    "\n",
    "    # Add arguments\n",
    "    parser.add_argument('--epochs', type=int, default=2000,\n",
    "                        help='Number of training epochs.')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.0003,\n",
    "                        help='Learning rate for the optimizer.')\n",
    "    parser.add_argument('--batch_size', type=int, default=1024,\n",
    "                        help='Batch size for training.')\n",
    "    parser.add_argument('--k_epochs', type=int, default=128,\n",
    "                        help='Number of policy update epochs per trajectory collection.')\n",
    "    parser.add_argument('--epsilon', type=float, default=0.2,\n",
    "                        help='Clipping parameter for PPO.')\n",
    "    parser.add_argument('--beta_kl', type=float, default=0.01,\n",
    "                        help='KL divergence coefficient (for PPO-like algorithms).')\n",
    "    parser.add_argument('--entropy_coeff', type=float, default=0.001,\n",
    "                        help='Entropy regularization coefficient.')\n",
    "    parser.add_argument('--log_iterations', type=int, default=100,\n",
    "                        help='Log training progress every N iterations.')\n",
    "    parser.add_argument('--gamma', type=float, default=0.99,\n",
    "                        help='Discount factor for rewards.')\n",
    "    parser.add_argument('--num_envs', type=int, default=16,\n",
    "                        help='Number of parallel environments for training.')\n",
    "    parser.add_argument('--use_cuda', action='store_true',\n",
    "                        help='Use CUDA if available.')\n",
    "    parser.add_argument('--device', type=str, default='cpu',\n",
    "                        help='Explicitly set device (e.g., \"cpu, cuda:0\", \"cpu\"). Overrides --use_cuda if specified.')\n",
    "    parser.add_argument('--save_model', action='store_true',\n",
    "                        help='Save the trained model weights.')\n",
    "    parser.add_argument('--model_output_path', type=str, default='blackjack_policy_model.pth',\n",
    "                        help='Path to save the trained model weights.')\n",
    "\n",
    "    # Parse the arguments\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    \n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRPO_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
