{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b1fe89",
   "metadata": {},
   "source": [
    "**BLACKJACK AGENT TRAINING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b061c",
   "metadata": {},
   "source": [
    "**Part 1**\n",
    "\n",
    "* Attempting to use Softmax (Categorical Distribution) implementation instead of Sigmoid (Binary Bernoulli Distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3399fe9",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "Took 32 minutes to run on CPU\n",
    "\n",
    "Parameters:\n",
    "\n",
    "epochs=2000,\n",
    "        learning_rate=0.0003,\n",
    "        batch_size=2048, # Significantly larger batch size recommended for stability\n",
    "        k_epochs=128,\n",
    "        epsilon=0.2,\n",
    "        beta_kl=0.01,\n",
    "        entropy_coeff=0.001,\n",
    "        log_iterations=100,\n",
    "        gamma=0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d09e02",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1301783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import random\n",
    "# import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c997ec",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83efa5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", sab=True) # `render_mode=\"human\"` creates a pygame popup window to analyze play # `sab=True` uses the Sutton & Barto version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7064b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the Environment, and get an observation\n",
    "obs, _ = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ca4afb",
   "metadata": {},
   "source": [
    "Observation Space\n",
    "* player_sum: The sum of the player's cards (integer between 4 and 21+).\n",
    "* dealer_card: The value of the dealer's visible card (1–10).\n",
    "* usable_ace: True if the player has a usable ace (counts as 11), otherwise False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a031b3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 2, 0)\n"
     ]
    }
   ],
   "source": [
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de0cf7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74644b73",
   "metadata": {},
   "source": [
    "* obs: New observation after the action.\n",
    "* reward: Final reward: +1 for win, 0 for draw, -1 for loss.\n",
    "* done: Whether the episode has ended.\n",
    "* truncated: Whether the episode was truncated (usually False here).\n",
    "* info: Extra info (often empty in Blackjack)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94ef5431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdd52ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d27e0",
   "metadata": {},
   "source": [
    "The Blackjack action space is Discrete(2):\n",
    "* 0 = Stick\n",
    "* 1 = Hit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a083c10b",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "547b9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackJackAgent(nn.Module):\n",
    "    def __init__(self, obs_size=3, hidden_size=10, output_size=2):\n",
    "        super(BlackJackAgent, self).__init__()\n",
    "        self.layer_1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.layer_2 = nn.Linear(hidden_size, output_size)\n",
    "        self.action_probs_activation_layer = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer_1(x))\n",
    "        logits = self.layer_2(x)\n",
    "        return logits       # later use nn.Softmax to get probabilities\n",
    "\n",
    "    def get_action_probs(self, logits):\n",
    "        \"\"\"Get the probabilities of each action.\"\"\"\n",
    "        return self.action_probs_activation_layer(logits)\n",
    "    \n",
    "    def sample_best_action(self, obs):\n",
    "        \"\"\"Get the deterministic action with the highest probability\n",
    "        for a given observation.\n",
    "        \n",
    "        Parameters:\n",
    "            obs (torch.tensor): the agent's current observable state in the playable environment. Expected shape is either `(num_features,)` for a single observation\n",
    "            or `(batch_size, num_features)` for a batch of observations.\n",
    "        \n",
    "        Returns:\n",
    "            action (int or torch.tensor): \n",
    "                - If `obs` is a single observation (i.e., `obs.dim() == 1`), returns a scalar `int` representing the chosen action. \n",
    "\n",
    "                - If `obs` is a batch of observations (i.e., `obs.dim() > 1`),\n",
    "                returns a `torch.Tensor` of `int`s, where each element is the\n",
    "                chosen action for the corresponding observation in the batch\"\"\"\n",
    "        # Ensure observation is a tensor and has a batch dimension if it's a single observation\n",
    "        if obs.dim() == 1:\n",
    "            obs = obs.unsqueeze(0) # Add a batch dimension if it's a single observation\n",
    "\n",
    "        logits = self.forward(obs)\n",
    "        probs = self.get_action_probs(logits)\n",
    "        action = torch.argmax(probs, dim=1) \n",
    "        if obs.size(0) == 1:    # This method checks if there is only 1 element in a 1-D tensor\n",
    "            return action.item() # Returns a Python scalar for a single observation\n",
    "        else:\n",
    "            return action # Returns a tensor of actions for a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618481e2",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b75f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_blackjack_agent(epochs=50, learning_rate=0.0001, batch_size=64, gamma=0.99, k_epochs=64, epsilon=0.2, beta_kl=0.01, max_grad_norm=0.5, entropy_coeff=0.01, log_iterations=10) -> BlackJackAgent:\n",
    "    print(f\"Training BlackJack Agent's Policy with {epochs} epochs, {learning_rate} learning rate, batch size {batch_size}, and KL beta {beta_kl}.\")\n",
    "    env = gym.make(\"Blackjack-v1\", sab=True) # # `sab=True` uses the Sutton & Barto version\n",
    "    New_Policy = BlackJackAgent()   # STEP 1 || CREATE π_new\n",
    "    optimizer = optim.Adam(params=New_Policy.parameters(), lr=learning_rate)\n",
    "\n",
    "    # STEP 2 || FOR I ITERATION STEPS OMITTED \n",
    "    # STEP 3 || CREATE REFERENCE MODEL OMITTED\n",
    "    for epoch in tqdm(range(epochs), desc=f\"Main Epoch (Outer Loop)\", leave=False):     # STEP 4 || FOR M ITERATION STEPS\n",
    "        \n",
    "        batch_trajectories = []     # Will contain a batch of trajectories\n",
    "\n",
    "        # STEP 5 || Sample a batch D_b from D --> OMITTED \n",
    "        # STEP 6 || Update the old policy model π_old <- π_new\n",
    "        Policy_Old = BlackJackAgent()\n",
    "        Policy_Old.load_state_dict(New_Policy.state_dict())\n",
    "        Policy_Old.eval()   # Prevent Gradient tracking\n",
    "\n",
    "        # --- STEP 7 || Collect a Batch of Experiences ---\n",
    "        # Loop Agent prediction, recording trajectories to lists:\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            # Create local episode trajectory library\n",
    "            episode_trajectory = {\"states\": [], \"actions\": [], \"rewards\": [], \"log_probs\": []}\n",
    "            obs, _ = env.reset()\n",
    "            done, truncated = False, False\n",
    "            while not done and not truncated:\n",
    "                obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0) # add batch dim to feed to NN; TENSOR SHAPE=(1, 3)\n",
    "                with torch.no_grad():\n",
    "                    logits = Policy_Old(obs_tensor)\n",
    "                    dist = torch.distributions.Categorical(logits=logits) # Create a Stochastic Distribution to sample from\n",
    "                    action = dist.sample() # Tensor of shape (1,1)\n",
    "                    log_prob = dist.log_prob(action)    # Tensor of shape (1, 1) > Tensor of shape(1,)\n",
    "                # list, float, boolean, boolean, dict    \n",
    "                next_obs, reward, done, truncated, info = env.step(action.item())\n",
    "\n",
    "                # Store completed episode_trajectory information\n",
    "                episode_trajectory[\"states\"].append(obs)\n",
    "                episode_trajectory[\"actions\"].append(action.item())\n",
    "                episode_trajectory[\"rewards\"].append(reward)\n",
    "                episode_trajectory[\"log_probs\"].append(log_prob)\n",
    "                \n",
    "                obs = next_obs  # Update the observation\n",
    "                if (truncated):\n",
    "                    print(\"Debug: EPISODE TRUNCATED\")\n",
    "            # Add completed episode information into Batch of Experiences \n",
    "            batch_trajectories.append(episode_trajectory)\n",
    "\n",
    "        # These lists will hold data from ALL episodes in the current batch for Advantage Calculation\n",
    "        all_states, all_actions, all_old_log_probs, all_discounted_rewards= [], [], [], []\n",
    "\n",
    "        # STEP 8 || Calculate Discounted Rewards\n",
    "        for episode_trajectory in batch_trajectories:   # Loop through all the episode trajectories\n",
    "            rewards = episode_trajectory[\"rewards\"]\n",
    "            states = episode_trajectory[\"states\"]\n",
    "            actions = episode_trajectory[\"actions\"]\n",
    "            log_probs = episode_trajectory[\"log_probs\"]\n",
    "            \n",
    "            discounted_reward = 0\n",
    "            returns_for_episode = []\n",
    "            for reward in reversed(rewards):\n",
    "                discounted_reward = reward + gamma * discounted_reward\n",
    "                returns_for_episode.insert(0, discounted_reward)\n",
    "\n",
    "            # Add each trajectory information for the batch if the states list is populated\n",
    "            if states:\n",
    "                all_states.extend(states)\n",
    "                all_actions.extend(actions)\n",
    "                all_old_log_probs.extend(log_probs)\n",
    "                all_discounted_rewards.extend(returns_for_episode)  # keep appending to the discounted rewards list\n",
    "\n",
    "\n",
    "        # --- Debugging: Print lengths and samples of collected data ---\n",
    "        # print(f\"DEBUG (Epoch {epoch + 1}): Length of all_states: {len(all_states)}\")\n",
    "        # print(f\"DEBUG (Epoch {epoch + 1}): Length of all_actions: {len(all_actions)}\")\n",
    "        # print(f\"DEBUG (Epoch {epoch + 1}): Length of all_old_log_probs (list): {len(all_old_log_probs)}\")\n",
    "        # print(f\"DEBUG (Epoch {epoch + 1}): Length of all_advantages (list): {len(all_discounted_rewards)}\")\n",
    "        # Example content check (first 2 elements if available)\n",
    "        # --- End Debugging ---\n",
    "\n",
    "\n",
    "        # --- IMPORTANT: Pre-tensorization checks and conversions ---\n",
    "        # Check if any essential list is empty before converting to tensors.\n",
    "        # This prevents RuntimeError due to dimension mismatches with empty tensors.\n",
    "        if not all_states or not all_actions or not all_old_log_probs or not all_discounted_rewards:\n",
    "            print(f\"Warning: Epoch {epoch + 1}: Insufficient data collected for optimization. \"\n",
    "                f\"Skipping policy update for this epoch.\")\n",
    "            # Print specific counts to help diagnose which list is empty\n",
    "            print(f\"  Counts: States={len(all_states)}, Actions={len(all_actions)}, \"\n",
    "                f\"LogProbs={len(all_old_log_probs)}, Advantages={len(all_discounted_rewards)}\")\n",
    "            continue # Skip to the next epoch if no meaningful data was collected\n",
    "\n",
    "\n",
    "        # Convert all collected batch list data into PyTorch tensors\n",
    "        all_states_tensor = torch.tensor(all_states, dtype=torch.float32)    # Shape: (batch_size, 3)\n",
    "        all_actions_tensor = torch.tensor(all_actions, dtype=torch.long)    # Shape: (batch_size, )\n",
    "        # Stack individual log_prob tensors from the batch of episodes and then flatten\n",
    "        all_old_log_probs_tensor = torch.cat(all_old_log_probs).squeeze(-1) # Create a long 1-D tensor of all the log probs using the 'old' policy; Resulting shape: (batch_size,)\n",
    "        all_discounted_rewards_tensor = torch.tensor(all_discounted_rewards, dtype=torch.float32)    # Shape: (Total_Steps_in_Batch,)\n",
    "\n",
    "        # STEP 9 || Calculate the Advantage of each Time Step for each Trajectory using normalization\n",
    "        all_advantages_tensor = (all_discounted_rewards_tensor - all_discounted_rewards_tensor.mean()) / (all_discounted_rewards_tensor.std() + 1e-8)\n",
    "\n",
    "        # Detach these tensors from any computation graph history\n",
    "        # as they represent fixed data for the policy updates in k_epochs.\n",
    "        # This prevents the \"RuntimeError: Trying to backward through the graph a second time\".\n",
    "        all_states_tensor = all_states_tensor.detach()\n",
    "        all_actions_tensor = all_actions_tensor.detach()\n",
    "        all_old_log_probs_tensor = all_old_log_probs_tensor.detach()\n",
    "        all_advantages_tensor = all_advantages_tensor.detach()\n",
    "\n",
    "        New_Policy.train()  # prepare NN for updates\n",
    "        loss_hist = []  # Track the loss of each optimization step\n",
    "\n",
    "        # --- STEP 10 || GRPO Optimization ---\n",
    "        for k_epoch in tqdm(range(k_epochs), desc=f\"Epoch {epoch+1}/{epochs} (Inner K-Epochs)\", leave=False):\n",
    "            new_logits = New_Policy(all_states_tensor)\n",
    "            new_dist = torch.distributions.Categorical(logits=new_logits)\n",
    "            new_log_probs = new_dist.log_prob(all_actions_tensor)\n",
    "            entropy = new_dist.entropy().mean() # Calculate entropy for regularization\n",
    "\n",
    "            R1_ratios = torch.exp(new_log_probs - all_old_log_probs_tensor)  # Exponent trick\n",
    "\n",
    "            unclipped_surrogates = R1_ratios * all_advantages_tensor \n",
    "            clipped_surrogates = torch.clamp(input=R1_ratios, min=1.0-epsilon, max=1.0+epsilon) * all_advantages_tensor\n",
    "\n",
    "            policy_loss = -torch.min(unclipped_surrogates, clipped_surrogates).mean()\n",
    "\n",
    "            # --- KL Divergence Calculation ---\n",
    "            # Create distributions for old policies using the trajectory states\n",
    "            with torch.no_grad():\n",
    "                old_logits = Policy_Old(all_states_tensor)\n",
    "            old_dist = torch.distributions.Categorical(logits=old_logits)\n",
    "\n",
    "            # Calculate KL divergence per sample, then take the mean over the batch\n",
    "            kl_div_per_sample = torch.distributions.kl.kl_divergence(p=new_dist, q=old_dist)\n",
    "            kl_loss = kl_div_per_sample.mean() # Mean over the batch\n",
    "\n",
    "            # Calculate Total Loss for GRPO step and store it in the loss history\n",
    "            total_loss = policy_loss + beta_kl * kl_loss - entropy_coeff * entropy\n",
    "            loss_hist.append(total_loss)\n",
    "\n",
    "            # STEP 11 || Policy Updates\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(New_Policy.parameters(), max_norm=max_grad_norm)\n",
    "            optimizer.step()    # Update policy parameters using gradient ascent\n",
    "        \n",
    "        \n",
    "        # --- Logging Metrics ---\n",
    "        if (epoch + 1) % log_iterations == 0:\n",
    "            # 1. Concatenate all loss tensors into one tensor, ensuring they are detached  to prevent gradient tracking\n",
    "            losses_tensor = torch.stack([loss.detach().cpu() for loss in loss_hist])    # Shape: (N,)\n",
    "\n",
    "            # 2. Calculate the mean of the concatenated tensor\n",
    "            mean_loss = losses_tensor.mean()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Mean Loss: {mean_loss.item():.4f}, Mean Ratio: {R1_ratios.detach().mean().item():.5f}, Entropy Term: {entropy:.5f}\")\n",
    "\n",
    "            avg_reward = sum(sum(ep[\"rewards\"]) for ep in batch_trajectories) / batch_size\n",
    "            print(f\"Average reward per episode in batch: {avg_reward:.2f}\")\n",
    "\n",
    "    New_Policy.eval()\n",
    "\n",
    "    env.close() # Close the environment after training\n",
    "    print(\"Training complete.\")\n",
    "    return New_Policy # Return the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5009b242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BlackJack Agent's Policy with 50 epochs, 0.0001 learning rate, batch size 64, and KL beta 0.01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  20%|██        | 10/50 [00:01<00:04,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Mean Loss: -0.0474, Mean Ratio: 1.01407, Entropy Term: 0.67201\n",
      "Average reward per episode in batch: -0.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  40%|████      | 20/50 [00:02<00:03,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Mean Loss: -0.0099, Mean Ratio: 0.99448, Entropy Term: 0.30686\n",
      "Average reward per episode in batch: -0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  60%|██████    | 30/50 [00:03<00:02,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Mean Loss: -0.0045, Mean Ratio: 1.00595, Entropy Term: 0.27528\n",
      "Average reward per episode in batch: -0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  80%|████████  | 40/50 [00:04<00:01,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Mean Loss: -0.0054, Mean Ratio: 1.00265, Entropy Term: 0.33185\n",
      "Average reward per episode in batch: -0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Mean Loss: -0.0171, Mean Ratio: 0.98943, Entropy Term: 0.38757\n",
      "Average reward per episode in batch: -0.16\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "_ = training_blackjack_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714c80d",
   "metadata": {},
   "source": [
    "Training BlackJack Agent's Policy with 10 epochs, 0.0001 learning rate, batch size 4, and KL beta 0.01.\n",
    "* Batch of Trajectories:\n",
    "* [{'states': [(12, 10, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1239])]}, \n",
    "* {'states': [(20, 7, 0)], 'actions': [0], 'rewards': [1.0], 'log_probs': [tensor([-0.0815])]}, \n",
    "* {'states': [(12, 1, 0), (17, 1, 0)], 'actions': [1, 1], 'rewards': [0.0, -1.0], 'log_probs': [tensor([-1.5968]), tensor([-1.9474])]}, \n",
    "* {'states': [(6, 6, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.2144])]}, \n",
    "* {'states': [(7, 4, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.2734])]}, \n",
    "* {'states': [(13, 3, 1)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1471])]}, \n",
    "* {'states': [(15, 10, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1000])]}, \n",
    "* {'states': [(12, 10, 0)], 'actions': [0], 'rewards': [1.0], 'log_probs': [tensor([-0.1239])]}, \n",
    "* {'states': [(14, 7, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1320])]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc91f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BlackJack Agent's Policy with 2000 epochs, 0.0003 learning rate, batch size 2048, and KL beta 0.01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):   5%|▌         | 100/2000 [01:34<37:02,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/2000, Mean Loss: -0.0008, Mean Ratio: 1.00103, Entropy Term: 0.15603\n",
      "Average reward per episode in batch: -0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  10%|█         | 200/2000 [03:22<38:28,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/2000, Mean Loss: -0.0016, Mean Ratio: 1.00027, Entropy Term: 0.12882\n",
      "Average reward per episode in batch: -0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  15%|█▌        | 300/2000 [05:33<38:31,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/2000, Mean Loss: -0.0005, Mean Ratio: 0.99999, Entropy Term: 0.10107\n",
      "Average reward per episode in batch: -0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  20%|██        | 400/2000 [07:57<34:28,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/2000, Mean Loss: -0.0009, Mean Ratio: 0.99992, Entropy Term: 0.09868\n",
      "Average reward per episode in batch: -0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  25%|██▌       | 500/2000 [10:07<33:37,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/2000, Mean Loss: -0.0019, Mean Ratio: 0.99884, Entropy Term: 0.09913\n",
      "Average reward per episode in batch: -0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  30%|███       | 600/2000 [12:20<32:46,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600/2000, Mean Loss: -0.0013, Mean Ratio: 0.99946, Entropy Term: 0.08959\n",
      "Average reward per episode in batch: -0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  35%|███▌      | 700/2000 [14:43<32:49,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700/2000, Mean Loss: -0.0012, Mean Ratio: 0.99922, Entropy Term: 0.08814\n",
      "Average reward per episode in batch: -0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  40%|████      | 800/2000 [17:01<26:28,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800/2000, Mean Loss: -0.0008, Mean Ratio: 0.99920, Entropy Term: 0.08933\n",
      "Average reward per episode in batch: -0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  45%|████▌     | 900/2000 [19:14<24:06,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 900/2000, Mean Loss: -0.0015, Mean Ratio: 0.99796, Entropy Term: 0.07866\n",
      "Average reward per episode in batch: -0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  50%|█████     | 1000/2000 [21:32<23:45,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/2000, Mean Loss: -0.0006, Mean Ratio: 1.00083, Entropy Term: 0.08190\n",
      "Average reward per episode in batch: -0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  55%|█████▌    | 1100/2000 [23:54<16:57,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1100/2000, Mean Loss: -0.0017, Mean Ratio: 0.99970, Entropy Term: 0.08072\n",
      "Average reward per episode in batch: -0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  60%|██████    | 1200/2000 [25:27<12:28,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1200/2000, Mean Loss: -0.0013, Mean Ratio: 0.99900, Entropy Term: 0.07769\n",
      "Average reward per episode in batch: -0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  65%|██████▌   | 1300/2000 [27:00<10:55,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1300/2000, Mean Loss: -0.0012, Mean Ratio: 0.99976, Entropy Term: 0.07350\n",
      "Average reward per episode in batch: -0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  70%|███████   | 1400/2000 [28:36<09:08,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1400/2000, Mean Loss: -0.0015, Mean Ratio: 0.99885, Entropy Term: 0.08780\n",
      "Average reward per episode in batch: -0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  75%|███████▌  | 1500/2000 [30:11<07:42,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1500/2000, Mean Loss: -0.0008, Mean Ratio: 1.00010, Entropy Term: 0.07182\n",
      "Average reward per episode in batch: -0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  80%|████████  | 1600/2000 [31:46<06:15,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1600/2000, Mean Loss: -0.0013, Mean Ratio: 0.99853, Entropy Term: 0.07718\n",
      "Average reward per episode in batch: -0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  85%|████████▌ | 1700/2000 [33:20<04:44,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1700/2000, Mean Loss: -0.0017, Mean Ratio: 0.99882, Entropy Term: 0.08602\n",
      "Average reward per episode in batch: -0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  90%|█████████ | 1800/2000 [35:00<04:16,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1800/2000, Mean Loss: -0.0012, Mean Ratio: 0.99997, Entropy Term: 0.08081\n",
      "Average reward per episode in batch: -0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  95%|█████████▌| 1900/2000 [37:16<02:21,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1900/2000, Mean Loss: -0.0012, Mean Ratio: 0.99921, Entropy Term: 0.07588\n",
      "Average reward per episode in batch: -0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000/2000, Mean Loss: -0.0010, Mean Ratio: 0.99996, Entropy Term: 0.07215\n",
      "Average reward per episode in batch: -0.06\n",
      "Training complete.\n",
      "\n",
      "Testing the trained policy:\n",
      "Average reward over 1000 test episodes: -0.0780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Example usage (assuming you have a way to call this function, e.g., in a main block)\n",
    "if __name__ == '__main__':\n",
    "    # You can adjust these parameters as needed\n",
    "    # Using a larger batch_size for more stable training and to reduce empty batch issues\n",
    "    trained_policy = training_blackjack_agent(\n",
    "        epochs=2000,\n",
    "        learning_rate=0.0003,\n",
    "        batch_size=2048, # Significantly larger batch size recommended for stability\n",
    "        k_epochs=128,\n",
    "        epsilon=0.2,\n",
    "        beta_kl=0.01,\n",
    "        entropy_coeff=0.001,\n",
    "        log_iterations=100,\n",
    "        gamma=0.99\n",
    "    )\n",
    "\n",
    "    print(\"\\nTesting the trained policy:\")\n",
    "    test_env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "    total_test_rewards = 0\n",
    "    num_test_episodes = 1000\n",
    "\n",
    "    for _ in range(num_test_episodes):\n",
    "        obs, _ = test_env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        while not done and not truncated:\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                action = trained_policy.sample_best_action(obs_tensor)\n",
    "            obs, reward, done, truncated, _ = test_env.step(action)\n",
    "            episode_reward += reward\n",
    "        total_test_rewards += episode_reward\n",
    "\n",
    "    print(f\"Average reward over {num_test_episodes} test episodes: {total_test_rewards / num_test_episodes:.4f}\")\n",
    "    test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbfdd16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlackJackAgent(\n",
       "  (layer_1): Linear(in_features=3, out_features=10, bias=True)\n",
       "  (layer_2): Linear(in_features=10, out_features=2, bias=True)\n",
       "  (action_probs_activation_layer): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5134104",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(\"Blackjack-v1\", render_mode=\"rgb_array\", sab=True)\n",
    "total_test_rewards = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcafcfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_episodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8de95e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Blackjack Agent\n",
      "Resetting env for episode: 1\n",
      "obs_tensor: tensor([[11., 10.,  0.]]) || Action taken: 1\n",
      "obs_tensor: tensor([[15., 10.,  0.]]) || Action taken: 1\n",
      "obs_tensor: tensor([[20., 10.,  0.]]) || Action taken: 0\n",
      "Reward: 0.0 || Final Observation before reward: (20, 10, 0)\n",
      "Resetting env for episode: 2\n",
      "obs_tensor: tensor([[12.,  7.,  0.]]) || Action taken: 1\n",
      "obs_tensor: tensor([[19.,  7.,  0.]]) || Action taken: 0\n",
      "Reward: 1.0 || Final Observation before reward: (19, 7, 0)\n",
      "Resetting env for episode: 3\n",
      "obs_tensor: tensor([[15., 10.,  0.]]) || Action taken: 1\n",
      "Reward: -1.0 || Final Observation before reward: (25, 10, 0)\n",
      "Resetting env for episode: 4\n",
      "obs_tensor: tensor([[12., 10.,  0.]]) || Action taken: 1\n",
      "obs_tensor: tensor([[18., 10.,  0.]]) || Action taken: 0\n",
      "Reward: -1.0 || Final Observation before reward: (18, 10, 0)\n",
      "Resetting env for episode: 5\n",
      "obs_tensor: tensor([[13.,  3.,  0.]]) || Action taken: 0\n",
      "Reward: 1.0 || Final Observation before reward: (13, 3, 0)\n",
      "Resetting env for episode: 6\n",
      "obs_tensor: tensor([[12.,  2.,  0.]]) || Action taken: 1\n",
      "obs_tensor: tensor([[19.,  2.,  0.]]) || Action taken: 0\n",
      "Reward: 0.0 || Final Observation before reward: (19, 2, 0)\n",
      "Resetting env for episode: 7\n",
      "obs_tensor: tensor([[ 7., 10.,  0.]]) || Action taken: 1\n",
      "obs_tensor: tensor([[16., 10.,  0.]]) || Action taken: 1\n",
      "Reward: -1.0 || Final Observation before reward: (26, 10, 0)\n",
      "Resetting env for episode: 8\n",
      "obs_tensor: tensor([[18., 10.,  0.]]) || Action taken: 0\n",
      "Reward: -1.0 || Final Observation before reward: (18, 10, 0)\n",
      "Resetting env for episode: 9\n",
      "obs_tensor: tensor([[19.,  3.,  0.]]) || Action taken: 0\n",
      "Reward: 1.0 || Final Observation before reward: (19, 3, 0)\n",
      "Resetting env for episode: 10\n",
      "obs_tensor: tensor([[16.,  1.,  0.]]) || Action taken: 1\n",
      "obs_tensor: tensor([[21.,  1.,  0.]]) || Action taken: 0\n",
      "Reward: 1.0 || Final Observation before reward: (21, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Testing Blackjack Agent\")\n",
    "for episode in range(num_test_episodes):\n",
    "    print(f\"Resetting env for episode: {episode+1}\")\n",
    "    obs, _ = test_env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "    while not done and not truncated:\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = trained_policy.sample_best_action(obs_tensor)\n",
    "            print(f\"obs_tensor: {obs_tensor} || Action taken: {action}\")\n",
    "        obs, reward, done, truncated, _ = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "        if (truncated): print(\"truncated\")\n",
    "    print(f\"Reward: {episode_reward} || Final Observation before reward: {obs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e00810d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to safely terminate the gym environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df04fdf4",
   "metadata": {},
   "source": [
    "Currently the final state which reveals what the dealer ended up with in the end is not shown. By trying to access the dealer's final hand or by adding custom logging within the environment, you'll gain the critical information needed to definitively understand the why behind each reward."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRPO_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
