{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02b061c",
   "metadata": {},
   "source": [
    "**Part 1**\n",
    "\n",
    "* Attempting to use Softmax (Categorical Distribution) implementation instead of Sigmoid (Binary Bernoulli Distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3399fe9",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "Took 32 minutes to run on CPU\n",
    "\n",
    "Parameters:\n",
    "\n",
    "epochs=2000,\n",
    "        learning_rate=0.0003,\n",
    "        batch_size=2048, # Significantly larger batch size recommended for stability\n",
    "        k_epochs=128,\n",
    "        epsilon=0.2,\n",
    "        beta_kl=0.01,\n",
    "        entropy_coeff=0.001,\n",
    "        log_iterations=100,\n",
    "        gamma=0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d09e02",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1301783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import random\n",
    "# import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c997ec",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83efa5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\gymnasium\\envs\\registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='rgb' that is not in the possible render_modes (['human', 'rgb_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Blackjack-v1\", sab=True) # `render_mode=\"human\"` creates a pygame popup window to analyze play # `sab=True` uses the Sutton & Barto version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7064b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the Environment, and get an observation\n",
    "obs, _ = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ca4afb",
   "metadata": {},
   "source": [
    "Observation Space\n",
    "* player_sum: The sum of the player's cards (integer between 4 and 21+).\n",
    "* dealer_card: The value of the dealer's visible card (1–10).\n",
    "* usable_ace: True if the player has a usable ace (counts as 11), otherwise False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a031b3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2, 0)\n"
     ]
    }
   ],
   "source": [
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0cf7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74644b73",
   "metadata": {},
   "source": [
    "* obs: New observation after the action.\n",
    "* reward: Final reward: +1 for win, 0 for draw, -1 for loss.\n",
    "* done: Whether the episode has ended.\n",
    "* truncated: Whether the episode was truncated (usually False here).\n",
    "* info: Extra info (often empty in Blackjack)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ef5431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd52ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d27e0",
   "metadata": {},
   "source": [
    "The Blackjack action space is Discrete(2):\n",
    "* 0 = Stick\n",
    "* 1 = Hit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a083c10b",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "547b9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackJackAgent(nn.Module):\n",
    "    def __init__(self, obs_size=3, hidden_size=10, output_size=2):\n",
    "        super(BlackJackAgent, self).__init__()\n",
    "        self.layer_1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.layer_2 = nn.Linear(hidden_size, output_size)\n",
    "        self.action_probs_activation_layer = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer_1(x))\n",
    "        logits = self.layer_2(x)\n",
    "        return logits       # later use nn.Softmax to get probabilities\n",
    "\n",
    "    def get_action_probs(self, logits):\n",
    "        \"\"\"Get the probabilities of each action.\"\"\"\n",
    "        return self.action_probs_activation_layer(logits)\n",
    "    \n",
    "    def sample_action(self, action:None):\n",
    "        \"\"\"Get the probability of choosing the action\"\"\"\n",
    "        logits = self.forward(action)\n",
    "        probs = self.get_action_probs(logits)\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample().item()\n",
    "        prob_of_action = dist.log_prob(action)\n",
    "        return action, prob_of_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618481e2",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b75f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_blackjack_agent(epochs=50, learning_rate=0.0001, batch_size=64, gamma=0.99, k_epochs=64, epsilon=0.2, beta_kl=0.01, max_grad_norm=0.5, entropy_coeff=0.01, log_iterations=10) -> BlackJackAgent: \n",
    "    print(f\"Training BlackJack Agent's Policy with {epochs} epochs, {learning_rate} learning rate, batch size {batch_size}, and KL beta {beta_kl}.\")\n",
    "    env = gym.make(\"Blackjack-v1\", sab=True) # # `sab=True` uses the Sutton & Barto version\n",
    "    New_Policy = BlackJackAgent()   # STEP 3 || \n",
    "    optimizer = optim.Adam(params=New_Policy.parameters(), lr=learning_rate)\n",
    "    # num_correct = 0.0\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=f\"Main Epoch (Outer Loop)\", leave=False):     # STEP 4 || \n",
    "        # STEP 3 || CREATE REFERENCE MODEL OMITTED\n",
    "        batch_trajectories = []     # Will contain a batch of trajectories\n",
    "\n",
    "        # STEP 5 || Sample a batch D_b from D --> OMITTED \n",
    "        # STEP 6 || Update the old policy model PI old <- PI new\n",
    "        Policy_Old = BlackJackAgent()\n",
    "        Policy_Old.load_state_dict(New_Policy.state_dict())\n",
    "        Policy_Old.eval()   # Prevent Gradient tracking\n",
    "\n",
    "        # --- STEP 7 Collect a Batch of Experiences ---\n",
    "        # Loop Agent prediction, recording trajectories to lists:\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            # Create local trajectory library\n",
    "            episode_trajectory = {\"states\": [], \"actions\": [], \"rewards\": [], \"log_probs\": []}\n",
    "            obs, _ = env.reset()\n",
    "            done, truncated = False, False\n",
    "            while not done and not truncated:\n",
    "                obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0) # add batch dim to feed to NN\n",
    "                with torch.no_grad():\n",
    "                    logits = Policy_Old(obs_tensor)\n",
    "                    dist = torch.distributions.Categorical(logits=logits)\n",
    "                    action = dist.sample() # Tensor of shape [1]\n",
    "                    log_prob = dist.log_prob(action)\n",
    "                    \n",
    "                next_obs, reward, done, truncated, info = env.step(action.item())\n",
    "\n",
    "                # Store episode_Trajectory\n",
    "                episode_trajectory[\"states\"].append(obs)\n",
    "                episode_trajectory[\"actions\"].append(action.item())\n",
    "                episode_trajectory[\"rewards\"].append(reward)\n",
    "                episode_trajectory[\"log_probs\"].append(log_prob)\n",
    "                \n",
    "                obs = next_obs  # Update the observation\n",
    "                if (truncated):\n",
    "                    print(\"Debug: EPISODE TRUNCATED\")\n",
    "\n",
    "            batch_trajectories.append(episode_trajectory)\n",
    "\n",
    "            # print(f\"Batch of Trajectories at current epoch:{epoch}:\\n{batch_trajectories}\")\n",
    "\n",
    "\n",
    "        # These lists will hold data from ALL episodes in the current batch for Advantage Calculation\n",
    "        all_states = []\n",
    "        all_actions = []\n",
    "        all_old_log_probs = []\n",
    "        all_discounted_rewards = []\n",
    "\n",
    "        # STEP 8 || Calculate Discounted Rewards\n",
    "        for episode_trajectory in batch_trajectories:\n",
    "            rewards = episode_trajectory[\"rewards\"]\n",
    "            states = episode_trajectory[\"states\"]\n",
    "            actions = episode_trajectory[\"actions\"]\n",
    "            log_probs = episode_trajectory[\"log_probs\"]\n",
    "            \n",
    "            discounted_reward = 0\n",
    "            returns_for_episode = []\n",
    "            for reward in reversed(rewards):\n",
    "                discounted_reward = reward + gamma * discounted_reward\n",
    "                returns_for_episode.insert(0, discounted_reward)\n",
    "\n",
    "            discounted_rewards = torch.tensor(returns_for_episode, dtype=torch.float32)\n",
    "            # print(f\"discounted_rewards size: {discounted_rewards.size()}\")\n",
    "            # Add each trajectory information for the batch\n",
    "            if states:\n",
    "                all_states.extend(states)\n",
    "                all_actions.extend(actions)\n",
    "                all_old_log_probs.extend(log_probs)\n",
    "                all_discounted_rewards.extend(discounted_rewards.tolist())\n",
    "\n",
    "        # --- Debugging: Print lengths and samples of collected data ---\n",
    "        # print(f\"DEBUG (Epoch {epoch + 1}): Length of all_states: {len(all_states)}\")\n",
    "        # print(f\"DEBUG (Epoch {epoch + 1}): Length of all_actions: {len(all_actions)}\")\n",
    "        # print(f\"DEBUG (Epoch {epoch + 1}): Length of all_old_log_probs (list): {len(all_old_log_probs)}\")\n",
    "        # print(f\"DEBUG (Epoch {epoch + 1}): Length of all_advantages (list): {len(all_discounted_rewards)}\")\n",
    "        # Example content check (first 2 elements if available)\n",
    "        # --- End Debugging ---\n",
    "\n",
    "\n",
    "        # --- IMPORTANT: Pre-tensorization checks and conversions ---\n",
    "        # Check if any essential list is empty before converting to tensors.\n",
    "        # This prevents RuntimeError due to dimension mismatches with empty tensors.\n",
    "        if not all_states or not all_actions or not all_old_log_probs or not all_discounted_rewards:\n",
    "            print(f\"Warning: Epoch {epoch + 1}: Insufficient data collected for optimization. \"\n",
    "                f\"Skipping policy update for this epoch.\")\n",
    "            # Print specific counts to help diagnose which list is empty\n",
    "            print(f\"  Counts: States={len(all_states)}, Actions={len(all_actions)}, \"\n",
    "                f\"LogProbs={len(all_old_log_probs)}, Advantages={len(all_discounted_rewards)}\")\n",
    "            continue # Skip to the next epoch if no meaningful data was collected\n",
    "\n",
    "\n",
    "        # Convert all collected batch data into PyTorch tensors\n",
    "        all_states_tensor = torch.tensor(all_states, dtype=torch.float32)\n",
    "        all_actions_tensor = torch.tensor(all_actions, dtype=torch.long)\n",
    "        # Stack individual log_prob tensors and then flatten if necessary\n",
    "        all_old_log_probs_tensor = torch.cat(all_old_log_probs).squeeze(-1) # Ensure it's a 1D tensor\n",
    "        all_discounted_rewards_tensor = torch.tensor(all_discounted_rewards, dtype=torch.float32)\n",
    "\n",
    "        # STEP 9 || Calculate the Advantage of each Time Step for each Trajectory using normalization\n",
    "        all_advantages_tensor = (all_discounted_rewards_tensor - all_discounted_rewards_tensor.mean()) / (all_discounted_rewards_tensor.std() + 1e-8)\n",
    "\n",
    "        # Detach these tensors from any computation graph history\n",
    "        # as they represent fixed data for the policy updates in k_epochs.\n",
    "        # This prevents the \"RuntimeError: Trying to backward through the graph a second time\".\n",
    "        all_states_tensor = all_states_tensor.detach()\n",
    "        all_actions_tensor = all_actions_tensor.detach()\n",
    "        all_old_log_probs_tensor = all_old_log_probs_tensor.detach()\n",
    "        all_advantages_tensor = all_advantages_tensor.detach()\n",
    "\n",
    "        New_Policy.train()  # prepare NN for updates\n",
    "\n",
    "        # --- STEP 10 || GRPO Optimization ---\n",
    "        for k_epoch in tqdm(range(k_epochs), desc=f\"Epoch {epoch+1}/{epochs} (Inner K-Epochs)\", leave=False):\n",
    "            new_logits = New_Policy(all_states_tensor)\n",
    "            new_dist = torch.distributions.Categorical(logits=new_logits)\n",
    "            new_log_probs = new_dist.log_prob(all_actions_tensor)\n",
    "            entropy = new_dist.entropy().mean() # Calculate entropy for regularization\n",
    "\n",
    "            R1_ratio = torch.exp(new_log_probs - all_old_log_probs_tensor)\n",
    "\n",
    "            unclipped_surrogate = R1_ratio * all_advantages_tensor\n",
    "            clipped_surrogate = torch.clamp(input=R1_ratio, min=1.0-epsilon, max=1.0+epsilon) * all_advantages_tensor\n",
    "\n",
    "            policy_loss = -torch.min(unclipped_surrogate, clipped_surrogate).mean()\n",
    "\n",
    "            # --- KL Divergence Calculation ---\n",
    "            # Create distributions for old policies using the trajectory states\n",
    "            with torch.no_grad():\n",
    "                old_logits = Policy_Old(all_states_tensor)\n",
    "            old_dist = torch.distributions.Categorical(logits=old_logits)\n",
    "\n",
    "            # Calculate KL divergence per sample, then take the mean over the batch\n",
    "            kl_div_per_sample = torch.distributions.kl.kl_divergence(p=new_dist, q=old_dist)\n",
    "            kl_loss = kl_div_per_sample.mean() # Mean over the batch\n",
    "\n",
    "            # Total Loss for GRPO\n",
    "            total_loss = policy_loss + beta_kl * kl_loss - entropy_coeff * entropy\n",
    "\n",
    "            # STEP 11 || Policy Updates\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(New_Policy.parameters(), max_grad_norm)\n",
    "            optimizer.step()    # Update policy parameters using gradient ascent\n",
    "        \n",
    "        \n",
    "        # --- 4. Logging and Evaluation ---\n",
    "        if (epoch + 1) % log_iterations == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}, Ratio: {R1_ratio.mean().item():.5f}, Entropy Term: {entropy:.5f}\")\n",
    "            # You can add more evaluation metrics here, e.g., average reward per episode\n",
    "            # For Blackjack, the reward is often -1, 0, or 1.\n",
    "            avg_reward = sum(sum(ep[\"rewards\"]) for ep in batch_trajectories) / batch_size\n",
    "            print(f\"Average reward per episode in batch: {avg_reward:.2f}\")\n",
    "\n",
    "    New_Policy.eval()\n",
    "\n",
    "\n",
    "    env.close() # Close the environment after training\n",
    "    print(\"Training complete.\")\n",
    "    return New_Policy # Return the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5009b242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BlackJack Agent's Policy with 50 epochs, 0.0001 learning rate, batch size 64, and KL beta 0.01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  20%|██        | 10/50 [00:01<00:04,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: -0.0001, Ratio: 1.00003, Entropy Term: 0.00301\n",
      "Average reward per episode in batch: -1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  40%|████      | 20/50 [00:02<00:03,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Loss: -0.0001, Ratio: 1.00004, Entropy Term: 0.00269\n",
      "Average reward per episode in batch: -1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  60%|██████    | 30/50 [00:03<00:02,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Loss: -0.0000, Ratio: 1.00001, Entropy Term: 0.00103\n",
      "Average reward per episode in batch: -1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  80%|████████  | 40/50 [00:04<00:01,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Loss: 0.0000, Ratio: 1.00000, Entropy Term: 0.00018\n",
      "Average reward per episode in batch: -1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Loss: -0.0000, Ratio: 1.00000, Entropy Term: 0.00014\n",
      "Average reward per episode in batch: -1.00\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "_ = training_blackjack_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714c80d",
   "metadata": {},
   "source": [
    "Training BlackJack Agent's Policy with 10 epochs, 0.0001 learning rate, batch size 4, and KL beta 0.01.\n",
    "* Batch of Trajectories:\n",
    "* [{'states': [(12, 10, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1239])]}, \n",
    "* {'states': [(20, 7, 0)], 'actions': [0], 'rewards': [1.0], 'log_probs': [tensor([-0.0815])]}, \n",
    "* {'states': [(12, 1, 0), (17, 1, 0)], 'actions': [1, 1], 'rewards': [0.0, -1.0], 'log_probs': [tensor([-1.5968]), tensor([-1.9474])]}, \n",
    "* {'states': [(6, 6, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.2144])]}, \n",
    "* {'states': [(7, 4, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.2734])]}, \n",
    "* {'states': [(13, 3, 1)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1471])]}, \n",
    "* {'states': [(15, 10, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1000])]}, \n",
    "* {'states': [(12, 10, 0)], 'actions': [0], 'rewards': [1.0], 'log_probs': [tensor([-0.1239])]}, \n",
    "* {'states': [(14, 7, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1320])]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BlackJack Agent's Policy with 2000 epochs, 0.0003 learning rate, batch size 2048, and KL beta 0.01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):   5%|▌         | 100/2000 [01:22<26:50,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/2000, Loss: -0.0006, Ratio: 0.99979, Entropy Term: 0.13789\n",
      "Average reward per episode in batch: -0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  10%|█         | 200/2000 [02:51<26:27,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/2000, Loss: -0.0016, Ratio: 0.99964, Entropy Term: 0.13101\n",
      "Average reward per episode in batch: -0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  15%|█▌        | 300/2000 [04:20<24:55,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/2000, Loss: -0.0006, Ratio: 0.99976, Entropy Term: 0.10833\n",
      "Average reward per episode in batch: -0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  20%|██        | 400/2000 [05:51<23:54,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/2000, Loss: -0.0011, Ratio: 0.99944, Entropy Term: 0.09554\n",
      "Average reward per episode in batch: -0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  25%|██▌       | 500/2000 [07:21<24:08,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/2000, Loss: -0.0009, Ratio: 0.99999, Entropy Term: 0.08084\n",
      "Average reward per episode in batch: -0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  30%|███       | 600/2000 [08:57<21:50,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600/2000, Loss: -0.0012, Ratio: 0.99913, Entropy Term: 0.08575\n",
      "Average reward per episode in batch: -0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  35%|███▌      | 700/2000 [10:35<21:21,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700/2000, Loss: -0.0008, Ratio: 0.99960, Entropy Term: 0.08500\n",
      "Average reward per episode in batch: -0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  40%|████      | 800/2000 [12:13<19:11,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800/2000, Loss: -0.0008, Ratio: 0.99989, Entropy Term: 0.08592\n",
      "Average reward per episode in batch: -0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  45%|████▌     | 900/2000 [13:48<17:38,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 900/2000, Loss: -0.0015, Ratio: 0.99910, Entropy Term: 0.07945\n",
      "Average reward per episode in batch: -0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  50%|█████     | 1000/2000 [15:24<16:14,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/2000, Loss: -0.0007, Ratio: 1.00059, Entropy Term: 0.07863\n",
      "Average reward per episode in batch: -0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  55%|█████▌    | 1100/2000 [17:01<14:34,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1100/2000, Loss: -0.0007, Ratio: 0.99975, Entropy Term: 0.07765\n",
      "Average reward per episode in batch: -0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  60%|██████    | 1200/2000 [18:37<12:53,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1200/2000, Loss: -0.0015, Ratio: 0.99962, Entropy Term: 0.07291\n",
      "Average reward per episode in batch: -0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  65%|██████▌   | 1300/2000 [20:15<11:36,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1300/2000, Loss: -0.0007, Ratio: 0.99966, Entropy Term: 0.07271\n",
      "Average reward per episode in batch: -0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  70%|███████   | 1400/2000 [21:52<09:58,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1400/2000, Loss: -0.0016, Ratio: 0.99973, Entropy Term: 0.06591\n",
      "Average reward per episode in batch: -0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  75%|███████▌  | 1500/2000 [23:28<08:10,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1500/2000, Loss: -0.0006, Ratio: 0.99978, Entropy Term: 0.06209\n",
      "Average reward per episode in batch: -0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  80%|████████  | 1600/2000 [25:05<06:31,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1600/2000, Loss: -0.0011, Ratio: 0.99927, Entropy Term: 0.06259\n",
      "Average reward per episode in batch: -0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  85%|████████▌ | 1700/2000 [26:36<04:19,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1700/2000, Loss: -0.0014, Ratio: 0.99800, Entropy Term: 0.07207\n",
      "Average reward per episode in batch: -0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  90%|█████████ | 1800/2000 [28:04<02:54,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1800/2000, Loss: -0.0006, Ratio: 0.99890, Entropy Term: 0.06358\n",
      "Average reward per episode in batch: -0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):  95%|█████████▌| 1900/2000 [29:32<01:27,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1900/2000, Loss: -0.0017, Ratio: 0.99867, Entropy Term: 0.06062\n",
      "Average reward per episode in batch: -0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000/2000, Loss: -0.0013, Ratio: 0.99954, Entropy Term: 0.06310\n",
      "Average reward per episode in batch: -0.05\n",
      "Training complete.\n",
      "\n",
      "Testing the trained policy:\n",
      "Average reward over 1000 test episodes: -0.0560\n"
     ]
    }
   ],
   "source": [
    "# Example usage (assuming you have a way to call this function, e.g., in a main block)\n",
    "if __name__ == '__main__':\n",
    "    # You can adjust these parameters as needed\n",
    "    # Using a larger batch_size for more stable training and to reduce empty batch issues\n",
    "    trained_policy = training_blackjack_agent(\n",
    "        epochs=2000,\n",
    "        learning_rate=0.0003,\n",
    "        batch_size=2048, # Significantly larger batch size recommended for stability\n",
    "        k_epochs=128,\n",
    "        epsilon=0.2,\n",
    "        beta_kl=0.01,\n",
    "        entropy_coeff=0.001,\n",
    "        log_iterations=100,\n",
    "        gamma=0.99\n",
    "    )\n",
    "\n",
    "    print(\"\\nTesting the trained policy:\")\n",
    "    test_env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "    total_test_rewards = 0\n",
    "    num_test_episodes = 1000\n",
    "\n",
    "    for _ in range(num_test_episodes):\n",
    "        obs, _ = test_env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        while not done and not truncated:\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                logits = trained_policy(obs_tensor)\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                action = dist.sample()\n",
    "            obs, reward, done, truncated, _ = test_env.step(action.item())\n",
    "            episode_reward += reward\n",
    "        total_test_rewards += episode_reward\n",
    "\n",
    "    print(f\"Average reward over {num_test_episodes} test episodes: {total_test_rewards / num_test_episodes:.4f}\")\n",
    "    test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa240d62",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbfdd16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlackJackAgent(\n",
       "  (layer_1): Linear(in_features=3, out_features=10, bias=True)\n",
       "  (layer_2): Linear(in_features=10, out_features=2, bias=True)\n",
       "  (action_probs_activation_layer): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5134104",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlackjack-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m\"\u001b[39m, sab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m total_test_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"Blackjack-v1\", render_mode=\"rgb\", sab=True)\n",
    "total_test_rewards = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcafcfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_episodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de95e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting env for episode: 0\n",
      "obs_tensor: tensor([[13.,  2.,  0.]]) || Action taken: tensor([1])\n",
      "obs_tensor: tensor([[14.,  2.,  0.]]) || Action taken: tensor([1])\n",
      "Reward: -1.0 || Final Observation: (23, 2, 0)\n",
      "Resetting env for episode: 1\n",
      "obs_tensor: tensor([[10., 10.,  0.]]) || Action taken: tensor([1])\n",
      "obs_tensor: tensor([[20., 10.,  0.]]) || Action taken: tensor([0])\n",
      "Reward: 1.0 || Final Observation: (20, 10, 0)\n",
      "Resetting env for episode: 2\n",
      "obs_tensor: tensor([[18., 10.,  0.]]) || Action taken: tensor([0])\n",
      "Reward: -1.0 || Final Observation: (18, 10, 0)\n",
      "Resetting env for episode: 3\n",
      "obs_tensor: tensor([[12., 10.,  0.]]) || Action taken: tensor([1])\n",
      "Reward: -1.0 || Final Observation: (22, 10, 0)\n",
      "Resetting env for episode: 4\n",
      "obs_tensor: tensor([[21.,  9.,  1.]]) || Action taken: tensor([0])\n",
      "Reward: 1.0 || Final Observation: (21, 9, 1)\n",
      "Resetting env for episode: 5\n",
      "obs_tensor: tensor([[19.,  2.,  0.]]) || Action taken: tensor([0])\n",
      "Reward: 1.0 || Final Observation: (19, 2, 0)\n",
      "Resetting env for episode: 6\n",
      "obs_tensor: tensor([[13.,  5.,  1.]]) || Action taken: tensor([1])\n",
      "obs_tensor: tensor([[13.,  5.,  0.]]) || Action taken: tensor([1])\n",
      "obs_tensor: tensor([[18.,  5.,  0.]]) || Action taken: tensor([0])\n",
      "Reward: -1.0 || Final Observation: (18, 5, 0)\n",
      "Resetting env for episode: 7\n",
      "obs_tensor: tensor([[5., 9., 0.]]) || Action taken: tensor([1])\n",
      "obs_tensor: tensor([[15.,  9.,  0.]]) || Action taken: tensor([1])\n",
      "Reward: -1.0 || Final Observation: (24, 9, 0)\n",
      "Resetting env for episode: 8\n",
      "obs_tensor: tensor([[13., 10.,  0.]]) || Action taken: tensor([1])\n",
      "Reward: -1.0 || Final Observation: (23, 10, 0)\n",
      "Resetting env for episode: 9\n",
      "obs_tensor: tensor([[21., 10.,  1.]]) || Action taken: tensor([0])\n",
      "Reward: 1.0 || Final Observation: (21, 10, 1)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for episode in range(num_test_episodes):\n",
    "    print(f\"Resetting env for episode: {episode}\")\n",
    "    obs, _ = test_env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "    stored_obs=[]\n",
    "    while not done and not truncated:\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = trained_policy(obs_tensor)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            print(f\"obs_tensor: {obs_tensor} || Action taken: {action}\")\n",
    "        obs, reward, done, truncated, _ = test_env.step(action.item())\n",
    "        episode_reward += reward\n",
    "        if (truncated): print(\"truncated\")\n",
    "    print(f\"Reward: {episode_reward} || Final Observation: {obs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00810d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df04fdf4",
   "metadata": {},
   "source": [
    "Currently the final state which reveals what the dealer ended up with in the end is not shown. By trying to access the dealer's final hand or by adding custom logging within the environment, you'll gain the critical information needed to definitively understand the why behind each reward."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRPO_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
