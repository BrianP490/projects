{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02b061c",
   "metadata": {},
   "source": [
    "**Part 1**\n",
    "\n",
    "* Attempting to use Softmax (Categorical Distribution) implementation instead of Sigmoid (Binary Bernoulli Distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d09e02",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1301783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c997ec",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83efa5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", sab=True) # # `sab=True` uses the Sutton & Barto version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the Environment, and get an observation\n",
    "obs, _ = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ca4afb",
   "metadata": {},
   "source": [
    "Observation Space\n",
    "* player_sum: The sum of the player's cards (integer between 4 and 21+).\n",
    "* dealer_card: The value of the dealer's visible card (1â€“10).\n",
    "* usable_ace: True if the player has a usable ace (counts as 11), otherwise False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a031b3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10, 0)\n"
     ]
    }
   ],
   "source": [
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0cf7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74644b73",
   "metadata": {},
   "source": [
    "* obs: New observation after the action.\n",
    "* reward: Final reward: +1 for win, 0 for draw, -1 for loss.\n",
    "* done: Whether the episode has ended.\n",
    "* truncated: Whether the episode was truncated (usually False here).\n",
    "* info: Extra info (often empty in Blackjack)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ef5431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdd52ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d27e0",
   "metadata": {},
   "source": [
    "The Blackjack action space is Discrete(2):\n",
    "* 0 = Stick\n",
    "* 1 = Hit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a083c10b",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "547b9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackJackAgent(nn.Module):\n",
    "    def __init__(self, obs_size=3, hidden_size=10, output_size=2):\n",
    "        super(BlackJackAgent, self).__init__()\n",
    "        self.layer_1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.layer_2 = nn.Linear(hidden_size, output_size)\n",
    "        self.action_probs_activation_layer = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer_1(x))\n",
    "        logits = self.layer_2(x)\n",
    "        return logits       # later use nn.Softmax to get probabilities\n",
    "\n",
    "    def get_action_probs(self, logits):\n",
    "        \"\"\"Get the probabilities of each action.\"\"\"\n",
    "        return self.action_probs_activation_layer(logits)\n",
    "    \n",
    "    def sample_action(self, action:None):\n",
    "        \"\"\"Get the probability of choosing the action\"\"\"\n",
    "        logits = self.forward(action)\n",
    "        probs = self.get_action_probs(logits)\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample().item()\n",
    "        prob_of_action = dist.log_prob(action)\n",
    "        return action, prob_of_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618481e2",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b75f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_blackjack_agent(epochs=1000, learning_rate=0.0001, batch_size=64, gamma=0.99, k_epochs=64, epsilon=0.2, beta_kl=0.01, max_grad_norm=0.5, entropy_coeff=0.01, log_iterations=10) -> BlackJackAgent: \n",
    "    print(f\"Training BlackJack Agent's Policy with {epochs} epochs, {learning_rate} learning rate, batch size {batch_size}, and KL beta {beta_kl}.\")\n",
    "    env = gym.make(\"Blackjack-v1\", sab=True) # # `sab=True` uses the Sutton & Barto version\n",
    "    New_Policy = BlackJackAgent()   # STEP 3 || \n",
    "    optimizer = optim.Adam(params=New_Policy.parameters(), lr=learning_rate)\n",
    "    # num_correct = 0.0\n",
    "\n",
    "    for epoch in range(epochs):     # STEP 4 || \n",
    "        # STEP 3 || CREATE REFERENCE MODEL OMITTED\n",
    "        batch_trajectories = []     # Will contain a batch of trajectories\n",
    "\n",
    "        # STEP 5 || Sample a batch D_b from D --> OMITTED \n",
    "        # STEP 6 || Update the old policy model PI old <- PI new\n",
    "        Policy_Old = BlackJackAgent()\n",
    "        Policy_Old.load_state_dict(New_Policy.state_dict())\n",
    "        Policy_Old.eval()   # Prevent Gradient tracking\n",
    "\n",
    "        # --- STEP 7 Collect a Batch of Experiences ---\n",
    "        # Loop Agent prediction, recording trajectories to lists:\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            # Create local trajectory library\n",
    "            episode_trajectory = {\"states\": [], \"actions\": [], \"rewards\": [], \"log_probs\": []}\n",
    "            obs, _ = env.reset()\n",
    "            done, truncated = False, False\n",
    "            while not done and not truncated:\n",
    "                obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0) # add batch dim to feed to NN\n",
    "                logits = Policy_Old(obs_tensor)\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                action = dist.sample() # Tensor of shape [1]\n",
    "                log_prob = dist.log_prob(action)\n",
    "                next_obs, reward, done, truncated, info = env.step(action.item())\n",
    "\n",
    "                # Store episode_Trajectory\n",
    "                episode_trajectory[\"states\"].append(obs)\n",
    "                episode_trajectory[\"actions\"].append(action.item())\n",
    "                episode_trajectory[\"rewards\"].append(reward)\n",
    "                episode_trajectory[\"log_probs\"].append(log_prob)\n",
    "                \n",
    "                obs = next_obs  # Update the observation\n",
    "\n",
    "            batch_trajectories.append(episode_trajectory)\n",
    "\n",
    "            print(f\"Batch of Trajectories at current epoch:{epoch}:\\n{batch_trajectories}\")\n",
    "\n",
    "\n",
    "            # These lists will hold data from ALL episodes in the current batch for Advantage Calculation\n",
    "            all_states = []\n",
    "            all_actions = []\n",
    "            all_old_log_probs = []\n",
    "            all_discounted_rewards = []\n",
    "\n",
    "            # STEP 8 || Calculate Discounted Rewards\n",
    "            for episode_trajectory in batch_trajectories:\n",
    "                rewards = episode_trajectory[\"rewards\"]\n",
    "                states = episode_trajectory[\"states\"]\n",
    "                actions = episode_trajectory[\"actions\"]\n",
    "                log_probs = episode_trajectory[\"log_probs\"]\n",
    "                \n",
    "                discounted_reward = 0\n",
    "                rewards = []\n",
    "                for reward in reversed(rewards):\n",
    "                    discounted_reward = reward + gamma * discounted_reward\n",
    "                    rewards.insert(0, discounted_reward)\n",
    "                discounted_rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "\n",
    "                # Add each trajectory information for the batch\n",
    "                all_states.extend(states)\n",
    "                all_actions.extend(actions)\n",
    "                all_old_log_probs.extend(log_probs)\n",
    "                all_discounted_rewards.extend(discounted_rewards.tolist())\n",
    "\n",
    "            # Convert all collected batch data into PyTorch tensors\n",
    "            all_states_tensor = torch.tensor(all_states, dtype=torch.float32)\n",
    "            all_actions_tensor = torch.tensor(all_actions, dtype=torch.long)\n",
    "            # Stack individual log_prob tensors and then flatten if necessary\n",
    "            all_old_log_probs_tensor = torch.cat(all_old_log_probs).squeeze(-1) # Ensure it's a 1D tensor\n",
    "            all_discounted_rewards_tensor = torch.tensor(all_discounted_rewards, dtype=torch.float32)\n",
    "\n",
    "            # STEP 9 || Calculate the Advantage of each Time Step for each Trajectory using normalization\n",
    "            all_advantages_tensor = (all_discounted_rewards_tensor - all_discounted_rewards_tensor.mean()) / (all_discounted_rewards_tensor.std() + 1e-8)\n",
    "\n",
    "            # --- STEP 10 || GRPO Optimization ---\n",
    "            for k_epoch in tqdm(range(k_epochs), desc=f\"Epoch {epoch+1}/{epochs} (Inner K-Epochs)\", leave=False):\n",
    "                new_logits = New_Policy(all_states_tensor)\n",
    "                new_dist = torch.distributions.Categorical(logits=new_logits)\n",
    "                new_log_probs = new_dist.log_prob(all_actions_tensor)\n",
    "                entropy = new_dist.entropy().mean() # Calculate entropy for regularization\n",
    "\n",
    "                R1_ratio = torch.exp(new_log_probs - all_old_log_probs_tensor)\n",
    "\n",
    "                unclipped_surrogate = R1_ratio * all_advantages_tensor\n",
    "                clipped_surrogate = torch.clamp(input=R1_ratio, min=1.0-epsilon, max=1.0+epsilon) * all_advantages_tensor\n",
    "\n",
    "                policy_loss = -torch.min(unclipped_surrogate, clipped_surrogate).mean()\n",
    "\n",
    "                # --- KL Divergence Calculation ---\n",
    "                # Create distributions for old policies using the trajectory states\n",
    "                with torch.no_grad():\n",
    "                    old_logits = Policy_Old(all_states_tensor)\n",
    "                old_dist = torch.distributions.Categorical(logits=old_logits)\n",
    "\n",
    "                # Calculate KL divergence per sample, then take the mean over the batch\n",
    "                kl_div_per_sample = torch.distributions.kl.kl_divergence(p=new_dist, q=old_dist)\n",
    "                kl_loss = kl_div_per_sample.mean() # Mean over the batch\n",
    "\n",
    "                # Total Loss for GRPO\n",
    "                total_loss = policy_loss + beta_kl * kl_loss - entropy_coeff * entropy\n",
    "\n",
    "                # STEP 11 || Policy Updates\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(New_Policy.parameters(), max_grad_norm)\n",
    "                optimizer.step()    # Update policy parameters using gradient ascent\n",
    "            \n",
    "            \n",
    "            # --- 4. Logging and Evaluation ---\n",
    "            if (epoch + 1) % log_iterations == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}, Ratio: {R1_ratio:.5f}, Entropy Term: {entropy:.5f}\")\n",
    "                # You can add more evaluation metrics here, e.g., average reward per episode\n",
    "                # For Blackjack, the reward is often -1, 0, or 1.\n",
    "                avg_reward = sum(sum(ep[\"rewards\"]) for ep in batch_trajectories) / batch_size\n",
    "                print(f\"Average reward per episode in batch: {avg_reward:.2f}\")\n",
    "\n",
    "        New_Policy.eval()\n",
    "\n",
    "\n",
    "    env.close() # Close the environment after training\n",
    "    print(\"Training complete.\")\n",
    "    return New_Policy # Return the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5009b242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brianperez\\AppData\\Local\\Temp\\ipykernel_17864\\4126388125.py:81: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\bld\\libtorch_1746251577579\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  all_advantages_tensor = (all_discounted_rewards_tensor - all_discounted_rewards_tensor.mean()) / (all_discounted_rewards_tensor.std() + 1e-8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BlackJack Agent's Policy with 1000 epochs, 0.0001 learning rate, batch size 64, and KL beta 0.01.\n",
      "Batch of Trajectories at current epoch:0:\n",
      "[{'states': [(14, 4, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.0601], grad_fn=<SqueezeBackward1>)]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_blackjack_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 112\u001b[0m, in \u001b[0;36mtraining_blackjack_agent\u001b[1;34m(epochs, learning_rate, batch_size, gamma, k_epochs, epsilon, beta_kl, max_grad_norm, entropy_coeff, log_iterations)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# STEP 11 || Policy Updates\u001b[39;00m\n\u001b[0;32m    111\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 112\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(New_Policy\u001b[38;5;241m.\u001b[39mparameters(), max_grad_norm)\n\u001b[0;32m    114\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()    \u001b[38;5;66;03m# Update policy parameters using gradient ascent\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\brianperez\\AppData\\Local\\anaconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\brianperez\\AppData\\Local\\anaconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\brianperez\\AppData\\Local\\anaconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "_ = training_blackjack_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714c80d",
   "metadata": {},
   "source": [
    "Training BlackJack Agent's Policy with 10 epochs, 0.0001 learning rate, batch size 4, and KL beta 0.01.\n",
    "* Batch of Trajectories:\n",
    "* [{'states': [(12, 10, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1239])]}, \n",
    "* {'states': [(20, 7, 0)], 'actions': [0], 'rewards': [1.0], 'log_probs': [tensor([-0.0815])]}, \n",
    "* {'states': [(12, 1, 0), (17, 1, 0)], 'actions': [1, 1], 'rewards': [0.0, -1.0], 'log_probs': [tensor([-1.5968]), tensor([-1.9474])]}, \n",
    "* {'states': [(6, 6, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.2144])]}, \n",
    "* {'states': [(7, 4, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.2734])]}, \n",
    "* {'states': [(13, 3, 1)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1471])]}, \n",
    "* {'states': [(15, 10, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1000])]}, \n",
    "* {'states': [(12, 10, 0)], 'actions': [0], 'rewards': [1.0], 'log_probs': [tensor([-0.1239])]}, \n",
    "* {'states': [(14, 7, 0)], 'actions': [0], 'rewards': [-1.0], 'log_probs': [tensor([-0.1320])]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e43bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRPO_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
